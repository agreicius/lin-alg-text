<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-05-24T17:10:34-05:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Diagonalization</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" id="latex-macros" class="hidden-content" style="display:none">\(\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\F}{{\mathbb F}}
\newcommand{\HH}{{\mathbb H}}
\newcommand{\compose}{\circ}
\newcommand{\bolda}{{\mathbf a}}
\newcommand{\boldb}{{\mathbf b}}
\newcommand{\boldc}{{\mathbf c}}
\newcommand{\boldd}{{\mathbf d}}
\newcommand{\bolde}{{\mathbf e}}
\newcommand{\boldi}{{\mathbf i}}
\newcommand{\boldj}{{\mathbf j}}
\newcommand{\boldk}{{\mathbf k}}
\newcommand{\boldn}{{\mathbf n}}
\newcommand{\boldp}{{\mathbf p}}
\newcommand{\boldq}{{\mathbf q}}
\newcommand{\boldr}{{\mathbf r}}
\newcommand{\boldsymbol}{{\mathbf s}}
\newcommand{\boldt}{{\mathbf t}}
\newcommand{\boldu}{{\mathbf u}}
\newcommand{\boldv}{{\mathbf v}}
\newcommand{\boldw}{{\mathbf w}}
\newcommand{\boldx}{{\mathbf x}}
\newcommand{\boldy}{{\mathbf y}}
\newcommand{\boldz}{{\mathbf z}}
\newcommand{\boldzero}{{\mathbf 0}}
\newcommand{\boldmod}{\boldsymbol{ \bmod }}
\newcommand{\boldT}{{\mathbf T}}
\newcommand{\boldN}{{\mathbf N}}
\newcommand{\boldB}{{\mathbf B}}
\newcommand{\boldF}{{\mathbf F}}
\newcommand{\boldS}{{\mathbf S}}
\newcommand{\boldG}{{\mathbf G}}
\newcommand{\boldK}{{\mathbf K}}
\newcommand{\boldL}{{\mathbf L}}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\NS}{null}
\DeclareMathOperator{\RS}{row}
\DeclareMathOperator{\CS}{col}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\Lt}{Lt}
\DeclareMathOperator{\Lc}{Lc}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\Frob}{Frob}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\diver}{div}
\DeclareMathOperator{\flux}{flux}
\def\Gal{\operatorname{Gal}}
\def\ord{\operatorname{ord}}
\def\ML{\operatorname{M}}
\def\GL{\operatorname{GL}}
\def\PGL{\operatorname{PGL}}
\def\SL{\operatorname{SL}}
\def\PSL{\operatorname{PSL}}
\def\GSp{\operatorname{GSp}}
\def\PGSp{\operatorname{PGSp}}
\def\Sp{\operatorname{Sp}}
\def\PSp{\operatorname{PSp}}
\def\Aut{\operatorname{Aut}}
\def\Inn{\operatorname{Inn}}
\def\Hom{\operatorname{Hom}}
\def\End{\operatorname{End}}
\def\ch{\operatorname{char}}
\def\Zp{\Z/p\Z}
\def\Zm{\Z/m\Z}
\def\Zn{\Z/n\Z}
\def\Fp{\F_p}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\injects}{\hookrightarrow}
\newcommand{\bijects}{\leftrightarrow}
\newcommand{\isomto}{\overset{\sim}{\rightarrow}}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\mclass}[2][m]{[#2]_{#1}}
\newcommand{\val}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\valuation}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\anpoly}{a_nx^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\anmonic}{x^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\bmpoly}{b_mx^m+b_{m-1}x^{m-1}\cdots +b_1x+b_0}
\newcommand{\pder}[2]{\frac{\partial#1}{\partial#2}}
\renewcommand{\c}{\cancel}
\newcommand{\normalin}{\trianglelefteq}
\newcommand{\angvec}[1]{\langle #1\rangle}
\newcommand{\varpoly}[2]{#1_{#2}x^{#2}+#1_{#2-1}x^{#2-1}\cdots +#1_1x+#1_0}
\newcommand{\varpower}[1][a]{#1_0+#1_1x+#1_1x^2+\cdots}
\newcommand{\limasto}[2][x]{\lim_{#1\rightarrow #2}}
\def\ntoinfty{\lim_{n\rightarrow\infty}}
\def\xtoinfty{\lim_{x\rightarrow\infty}}
\def\ii{\item}
\def\bb{\begin{enumerate}}
\def\ee{\end{enumerate}}
\def\ds{\displaystyle}
\def\p{\partial}
\newcommand{\abcdmatrix}[4]{\begin{bmatrix}#1\amp #2\\ #3\amp #4 \end{bmatrix}
}
\newenvironment{amatrix}[1][ccc|c]{\left[\begin{array}{#1}}{\end{array}\right]}
\newenvironment{linsys}[2][m]{
\begin{array}[#1]{@{}*{#2}{rc}r@{}}
}{
\end{array}}
\newcommand{\eqsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\numeqsys}{\begin{array}{rrcrcrcr}
e_1:\amp  a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
e_2: \amp a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
e_m: \amp a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\homsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp 0\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp 0\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp 0
\end{array}
}
\newcommand{\vareqsys}[4]{
\begin{array}{ccccccc}
#3_{11}x_{1}\amp +\amp #3_{12}x_{2}\amp +\cdots+\amp  #3_{1#2}x_{#2}\amp =\amp #4_1\\
#3_{21}x_{1}\amp +\amp #3_{22}x_{2}\amp +\cdots+\amp #3_{2#2}x_{#2}\amp =\amp #4_2\\
\vdots \amp \amp \vdots \amp  \amp \vdots \amp =\amp  \vdots\\
#3_{#1 1}x_{1}\amp +\amp #3_{#1 2}x_{2}\amp +\cdots +\amp #3_{#1 #2}x_{#2}\amp =\amp #4_{#1}
\end{array}
}
\newcommand{\genmatrix}[1][a]{
\begin{bmatrix}
#1_{11} \amp  #1_{12} \amp  \cdots \amp  #1_{1n} \\
#1_{21} \amp  #1_{22} \amp  \cdots \amp  #1_{2n} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#1_{m1} \amp  #1_{m2} \amp  \cdots \amp  #1_{mn}
\end{bmatrix}
}
\newcommand{\varmatrix}[3]{
\begin{bmatrix}
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}
\end{bmatrix}
}
\newcommand{\augmatrix}{
\begin{amatrix}[cccc|c]
a_{11} \amp  a_{12} \amp  \cdots \amp  a_{1n} \amp b_{1}\\
a_{21} \amp  a_{22} \amp  \cdots \amp  a_{2n} \amp b_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
a_{m1} \amp  a_{m2} \amp  \cdots \amp  a_{mn}\amp b_{m}
\end{amatrix}
}
\newcommand{\varaugmatrix}[4]{
\begin{amatrix}[cccc|c]
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \amp #4_{1}\\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \amp #4_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}\amp #4_{#1}
\end{amatrix}
}
\newcommand{\spaceforemptycolumn}{\makebox[\wd\boxofmathplus]{\ }}
\newcommand{\grstep}[2][\relax]{
\mathrel{
\hspace{\grsteplength}\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
\begin{subarray}{l} #1 \end{subarray}}\hspace{\grsteplength}}}
\newcommand{\repeatedgrstep}[2][\relax]{\hspace{-\grsteplength}\grstep[#1]{#2}}
\newcommand{\swap}{\leftrightarrow}
\newcommand{\deter}[1]{ \mathchoice{\left|#1\right|}{|#1|}{|#1|}{|#1|} }
\newcommand{\generalmatrix}[3]{
\left(
\begin{array}{cccc}
#1_{1,1}  \amp #1_{1,2}  \amp \ldots  \amp #1_{1,#2}  \\
#1_{2,1}  \amp #1_{2,2}  \amp \ldots  \amp #1_{2,#2}  \\
\amp \vdots                         \\
#1_{#3,1} \amp #1_{#3,2} \amp \ldots  \amp #1_{#3,#2}
\end{array}
\right)  }
\newcommand{\colvec}[2][c]{\begin{bmatrix}[#1] #2 \end{bmatrix}}
\newcommand{\rowvec}[1]{\setlength{\arraycolsep}{3pt}\begin{bmatrix} #1 \end{bmatrix}}
\DeclareMathOperator{\trace}{tr}
\newcommand{\isomorphicto}{\cong}
\newcommand{\rangespace}[1]{ \mathscr{R}(#1) }
\newcommand{\nullspace}[1]{ \mathscr{N}(#1) }
\newcommand{\genrangespace}[1]{ \mathscr{R}_\infty(#1) }
\newcommand{\gennullspace}[1]{ \mathscr{N}_\infty(#1) }
\newcommand{\zero}{ \vec{0} }
\newcommand{\polyspace}{\mathcal{P}}
\newcommand{\matspace}{\mathcal{M}}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\adjoint}{adj}
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\definend}[1]{\emph{#1}}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\map}[3]{\mbox{$#1\colon #2\to #3$}}
\newcommand{\mapsunder}[1]{\stackrel{#1}{\longmapsto}}
\newcommand{\mapsvia}[1]{\xrightarrow{#1}}
\newcommand{\xmapsunder}[1]{\mapsunder{#1}}
\newcommand{\composed}[2]{#1\mathbin{\circ} #2}
\DeclareMathOperator{\identity}{id}
\newcommand{\restrictionmap}[2]{{#1}\mathpunct\upharpoonright\hbox{}_{#2}}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\setspacing}{0.1em}
\newcommand{\set}[1]{\mbox{$\{\hspace{\setspacing}#1\hspace{\setspacing}\}$}}
\newcommand{\suchthat}{\mid}
\newcommand{\union}{\cup}
\newcommand{\intersection}{\cap}
\newcommand{\sequence}[1]{ \langle#1\rangle }
\newcommand{\interval}[2]{#1\,\ldots\, #2}
\newcommand{\setinterval}[2]{\mbox{$\{\interval{#1}{#2}\}$}}
\newcommand{\openinterval}[2]{(\interval{#1}{#2})}
\newcommand{\closedinterval}[2]{[\interval{#1}{#2}]}
\newcommand{\clopinterval}[2]{[\interval{#1}{#2})}
\newcommand{\opclinterval}[2]{(\interval{#1}{#2}]}
\newcommand{\isimpliedby}{\Longleftarrow}
\newcommand{\Sage}{\textit{Sage}}
\newcommand{\Maple}{\textit{Maple}}
\newcommand{\cat}[2]{#1\!\mathbin{\raise.6ex\hbox{\left( {}^\frown \right)}}\!#2}
\newcommand{\alignedvdots}[1][10pt]{\mskip2.5mu\makebox[.5\equalsignwd][r]{}
\smash{\vdots}}
\newcommand{\stdbasis}{{\cal E}}
\newcommand{\basis}[2]{\sequence{\vec{#1}_1,\ldots,\vec{#1}_{#2}}}
\newcommand{\rowspace}[1]{ \mathop{{\mbox{Rowspace}}}(#1) }
\newcommand{\colspace}[1]{ \mathop{{\mbox{Columnspace}}}(#1) }
\newcommand{\linmaps}[2]{ \mathop{{\cal L}}(#1,#2) }
\newcommand{\lincombo}[2]{
#1_1#2_1+#1_2#2_2+\cdots +#1_n#2_n}
\newcommand{\rep}[2]{ { Rep}_{#2}(#1) }
\newcommand{\wrt}[1]{{\mbox{\scriptsize \textit{wrt}\hspace{.25em}\left( #1 \right)} }}
\newcommand{\trans}[1]{ {{#1}^{\mathsf{T}}} }
\newcommand{\proj}[2]{\mbox{proj}_{#2}({#1}) }
\newcommand{\spanof}[1]{\relax [#1\relax ]}
\newcommand{\directsum}{\oplus}
\DeclareMathOperator{\dist}{dist}
\newcommand{\nbyn}[1]{#1 \! \times \! #1 }
\newcommand{\nbym}[2]{#1 \! \times \! #2 }
\newcommand{\degs}[1]{#1^\circ\relax}
\newcommand{\votepreflist}[3]{\colvec{#1 \\ #2 \\ #3}}
\newcommand{\votinggraphic}[1]{\hspace{1.15em}\mathord{[.3in][.2in]{\includegraphics{voting.#1}}}\hspace{1.15em}}
\newcommand{\magicsquares}{\mathscr{M}}
\newcommand{\semimagicsquares}{\mathscr{H}}
\newcommand{\circuitfont}{\sffamily}
\newcommand{\digitinsq}[1]{\fbox{\left( #1 \right)} }
\newcommand{\matrixvenlarge}[1]{\vbox{
\vspace{\extramatrixvspace}
\hbox{$#1$}
\vspace{\extramatrixvspace}
}}
\def\bspace{
{\vspace{.05in}}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">Linear algebra: the theory of vector spaces</span></a></h1>
<p class="byline">Aaron Greicius</p>
</div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="ss_eigenvectors.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="c_transbasis.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="c_innerproductspaces.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="ss_eigenvectors.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="c_transbasis.html" title="Up">Up</a><a class="next-button button toolbar-item" href="c_innerproductspaces.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="c_linear_systems.html" data-scroll="c_linear_systems"><span class="codenumber">1</span> <span class="title">Linear systems of equations</span></a><ul>
<li><a href="s_systems.html" data-scroll="s_systems">Linear systems of equations</a></li>
<li><a href="s_ge.html" data-scroll="s_ge">Gaussian elimination</a></li>
<li><a href="s_solving.html" data-scroll="s_solving">Solving linear systems</a></li>
<li><a href="ss_matrix.html" data-scroll="ss_matrix">Matrix arithmetic</a></li>
<li><a href="ss_algebraic.html" data-scroll="ss_algebraic">Algebraic properties</a></li>
<li><a href="ss_elementary.html" data-scroll="ss_elementary">Elementary matrices</a></li>
<li><a href="ss_invertible.html" data-scroll="ss_invertible">More on invertibility</a></li>
</ul>
</li>
<li class="link">
<a href="c_det.html" data-scroll="c_det"><span class="codenumber">2</span> <span class="title">The determinant</span></a><ul>
<li><a href="ss_det.html" data-scroll="ss_det">The determinant</a></li>
<li><a href="ss_rowops.html" data-scroll="ss_rowops">Row operations and the determinant</a></li>
<li><a href="ss_further.html" data-scroll="ss_further">Further properties</a></li>
</ul>
</li>
<li class="link">
<a href="c_vectorspace.html" data-scroll="c_vectorspace"><span class="codenumber">3</span> <span class="title">Vector spaces and linear transformations</span></a><ul>
<li><a href="ss_vectorspace.html" data-scroll="ss_vectorspace">Real vector spaces</a></li>
<li><a href="ss_transformation.html" data-scroll="ss_transformation">Linear transformations</a></li>
<li><a href="ss_subspace.html" data-scroll="ss_subspace">Subspaces</a></li>
<li><a href="ss_independence.html" data-scroll="ss_independence">Linear independence</a></li>
<li><a href="ss_basis.html" data-scroll="ss_basis">Bases</a></li>
<li><a href="ss_dimension.html" data-scroll="ss_dimension">Dimension</a></li>
</ul>
</li>
<li class="link">
<a href="c_transbasis.html" data-scroll="c_transbasis"><span class="codenumber">4</span> <span class="title">Linear transformations, bases, and dimension</span></a><ul>
<li><a href="ss_rank-nullity.html" data-scroll="ss_rank-nullity">Linear transformations and bases</a></li>
<li><a href="ss_rank-nullity.html" data-scroll="ss_rank-nullity">Linear transformations and bases</a></li>
<li><a href="ss_coordinate.html" data-scroll="ss_coordinate">Coordinate vectors</a></li>
<li><a href="ss_matrixreps.html" data-scroll="ss_matrixreps">Matrix representations</a></li>
<li><a href="ss_changeofbasis.html" data-scroll="ss_changeofbasis">Change of basis</a></li>
<li><a href="ss_eigenvectors.html" data-scroll="ss_eigenvectors">Eigenvectors and eigenvalues</a></li>
<li><a href="ss_diagonalization.html" data-scroll="ss_diagonalization" class="active">Diagonalization</a></li>
</ul>
</li>
<li class="link">
<a href="c_innerproductspaces.html" data-scroll="c_innerproductspaces"><span class="codenumber">5</span> <span class="title">Inner product spaces</span></a><ul>
<li><a href="ss_innerproducts.html" data-scroll="ss_innerproducts">Inner products, norms, angles</a></li>
<li><a href="ss_orthogonality.html" data-scroll="ss_orthogonality">Orthogonality, Gram-Schmidt, orthogonal projection</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-defs.html" data-scroll="appendix-defs"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thms.html" data-scroll="appendix-thms"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-egs.html" data-scroll="appendix-egs"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="references-1.html" data-scroll="references-1"><span class="title">Bibliography</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="ss_diagonalization"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">4.7</span> <span class="title">Diagonalization</span>
</h2>
<article class="paragraphs" id="paragraphs-227"><h5 class="heading"><span class="title">Diagonalizable matrices.</span></h5>
<article class="definition definition-like" id="definition-47"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.7.1</span><span class="period">.</span>
</h6>
<p id="p-1076">An \(n\times n\) matrix \(A\) is <dfn class="terminology">diagonalizable</dfn> if there is an invertible matrix \(P\) such that \(D=P^{-1}AP\) is diagonal.</p>
<p id="p-1077">In other words, \(A\) is diagonalizable if it is <dfn class="terminology">similar</dfn> to a diagonal matrix \(D\text{.}\)</p></article><article class="remark remark-like" id="remark-8"><h6 class="heading">
<span class="type">Remark</span><span class="space"> </span><span class="codenumber">4.7.2</span><span class="period">.</span>
</h6>
<ol class="decimal">
<li id="li-251"><p id="p-1078">If \(A\) is itself diagonal, then it is diagonalizable: we may choose \(P=I_n\) in the definition.</p></li>
<li id="li-252"><p id="p-1079">In this section we will develop a systematic procedure for determining whether a matrix is diagonalizable. As we will see, the answer is yes if and only if \(A\) has “enough” linearly independent eigenvectors. Of course we will spell out precisely what we mean by “enough”.</p></li>
<li id="li-253"><p id="p-1080">Not all matrices are diagonalizable. For example, \(A=\begin{bmatrix}0\amp 1\\ 0\amp 0 \end{bmatrix}\) is not diagonalizable, as the aforementioned procedure will show.</p></li>
<li id="li-254"><p id="p-1081">Roughly speaking, you should interpret being diagonalizable as meaning “as good as diagonal”. To elaborate: doing arithmetic with diagonal matrices \(D\) is extremely easy; if we know \(A\) is diagonalizable, meaning it is similar to a diagonal matrix \(D\text{,}\) then it shares many essential properties of \(D\text{,}\) and we can use the relation \(D=P^{-1}AP\) to help ease arithmetic computations involving \(A\text{.}\)</p></li>
</ol></article></article><article class="paragraphs" id="paragraphs-228"><h5 class="heading"><span class="title">Properties of conjugation.</span></h5>
<p id="p-1082">If we have, \(D=P^{-1}AP\text{,}\) why exactly is there such a close connection between \(D\) and \(A\text{?}\) One explanation has to do with the underlying operation</p>
<div class="displaymath">
\begin{equation*}
A\longmapsto P^{-1}AP\text{,}
\end{equation*}
</div>
<p class="continuation">which we call <em class="emphasis">conjugation by \(P\)</em>. As the following theorem outlines, conjugation by an invertible \(P\) satisfies many useful properties, and we use these to relate the matrix \(A\) with the matrix \(P^{-1}AP\text{.}\)</p>
<article class="theorem theorem-like" id="th_conjugation"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.7.3</span><span class="period">.</span><span class="space"> </span><span class="title">Properties of conjugation.</span>
</h6>
<p id="p-1083">Let \(P\) be any invertible \(n\times n\) matrix.</p>
<ol class="decimal">
<li id="li-255"><p id="p-1084">\(P^{-1}(c_1A_1+c_2A_2)P=c_1P^{-1}A_1P+c_2P^{-1}A_2P\text{.}\) (\alert{Conjugation by \(P\) is a linear transformation}.)</p></li>
<li id="li-256"><p id="p-1085">\(P^{-1}A^nP=(P^{-1}AP)^n\) for any integer \(n\geq 0\text{.}\) If \(A\) is invertible, the equality holds for <em class="emphasis">all</em> integers \(n\text{.}\) (\alert{Conjugation preserves powers}.)</p></li>
<li id="li-257"><p id="p-1086">Recall that given any polynomial \(f(x)=\anpoly\) and any \(n\times n\) matrix \(A\) we define \(f(A)=a_nA^n+a_{n-1}A^{n-1}+a_1A+a_0I_n\text{.}\) We have \(f(P^{-1}AP)=P^{-1}f(A)P\) for any polynomial \(f(x)\text{,}\) and any \(n\times n\) matrix \(A\text{.}\) (\alert{Conjugation preserves polynomial evaluation}.)</p></li>
</ol></article><article class="hiddenproof" id="proof-49"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-49"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-49"><article class="hiddenproof"><p id="p-1087">Exercise.</p></article></div></article><article class="paragraphs" id="paragraphs-229"><h5 class="heading"><span class="title">Examples: utility of diagonalizability.</span></h5>
<p id="p-1088">Suppose \(A\) is diagonalizable, so that \(D=P^{-1}AP\text{,}\) where \(D\) is diagonal with diagonal entries \(d_i\text{.}\) Using <a class="xref" data-knowl="./knowl/th_conjugation.html" title="Theorem 4.7.3: Properties of conjugation">Theorem 4.7.3</a>, we can now see how to translate statements about \(D\) (which are generally easy to prove), to statements about \(A\) (which otherwise might have been difficult to show).</p>
<article class="example example-like" id="eg_diagpowers"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-eg_diagpowers"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.7.4</span><span class="period">.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-eg_diagpowers"><article class="example example-like"><p id="p-1089">To compute \(A^{n}\) (hard) we can just compute \(D^{n}\) (easy) and then observe that \(A=PDP^{-1}\text{,}\) and thus</p>
<div class="displaymath">
\begin{equation*}
A^{n}=(PDP^{-1})^{n}=PD^{n}P^{-1}\text{,}
\end{equation*}
</div>
<p class="continuation">where the last equality follows from part (b) of <a class="xref" data-knowl="./knowl/th_conjugation.html" title="Theorem 4.7.3: Properties of conjugation">Theorem 4.7.3</a>; here we let \(P^{-1}\) assume the role of \(P\) in the theorem statement.</p>
<p id="p-1090">For example, let \(A=\begin{bmatrix}1\amp 3\\ 1\amp -1 \end{bmatrix}\text{.}\) Let's compute \(A^n\) for arbitrary \(n\text{.}\)</p>
<p id="p-1091">We have \(D=P^{-1}AP\text{,}\) where \(D=\begin{bmatrix}2\amp 0\\ 0\amp -2 \end{bmatrix}\text{,}\) and \(P=\begin{bmatrix}3\amp 1\\ 1\amp -1 \end{bmatrix}\text{.}\) (This is not obvious yet. Soon we will have the tools to see why this is so.)</p>
<p id="p-1092">Then \(A=PDP^{-1}\text{,}\) and thus</p>
<div class="displaymath">
\begin{equation*}
A^{n}=PD^{n}P^{-1}=P\begin{bmatrix}2^{100}\amp 0\\ 0\amp (-2)^{100} \end{bmatrix} P^{-1}=\frac{1}{4}\begin{bmatrix}3\cdot2^n+(-2)^n\amp 3\cdot 2^n-3(-2)^{n}\\ 2^{n}-(-2)^n\amp 2^n+3(-2)^{n} \end{bmatrix}\text{.}
\end{equation*}
</div></article></div></article><article class="paragraphs" id="paragraphs-230"><h5 class="heading"><span class="title">Examples: utility of diagonalizability.</span></h5>
<p id="p-1093">Suppose \(A\) is diagonalizable, so that \(D=P^{-1}AP\text{,}\) where \(D\) is diagonal with diagonal entries \(d_i\text{.}\)</p>
<article class="example example-like" id="example-12"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-example-12"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.7.5</span><span class="period">.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-example-12"><article class="example example-like"><p id="p-1094">More generally, we have \(f(A)=f(PDP^{-1})=Pf(D)P^{-1}\) for any polynomial \(f(x)=\anpoly\text{.}\) Since \(D\) is diagonal, with diagonal entries \(d_i\text{,}\) it is easy to see that \(f(D)\) is also diagonal, with diagonal entries \(f(d_i)\text{.}\)</p>
<p id="p-1095">In particular we see that \(f(A)=\underset{n\times n}{\boldzero}\) if and only if \(f(D)=\underset{n\times n}{\boldzero}\text{,}\) and this holds if and only if \(f(d_i)=0\) for each diagonal entry \(d_i\) of \(D\text{.}\)</p>
<p id="p-1096">Take the matrix \(A\) from <a class="xref" data-knowl="./knowl/eg_diagpowers.html" title="Example 4.7.4">Example 4.7.4</a>, and let \(f(x)=(x-2)(x+2)=x^2-4\text{.}\) Since \(f(2)=f(-2)=0\text{,}\) it follows that \(f(D)=f(A)=\underset{2\times 2}{\boldzero}\text{.}\) In other words, \(A^2-4I=\underset{2\times 2}{\boldzero}\text{,}\) as you can easily check.</p></article></div></article><article class="paragraphs" id="paragraphs-231"><h5 class="heading"><span class="title">Examples: utility of diagonalizability.</span></h5>
<p id="p-1097">Suppose \(A\) is diagonalizable, so that \(D=P^{-1}AP\text{,}\) where \(D\) is diagonal with diagonal entries \(d_i\text{.}\)</p>
<article class="example example-like" id="example-13"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-example-13"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.7.6</span><span class="period">.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-example-13"><article class="example example-like"><p id="p-1098">\(A\) has a <em class="emphasis">square-root</em> (i.e., a matrix \(B\) such that \(B^2=A\)) iff \(D\) has a square-root.</p>
<p id="p-1099">Indeed, suppose \(B^2=A\text{.}\) Set \(C=P^{-1}BP\text{.}\) Then \(C^2=P^{-1}B^2P=P^{-1}AP=D\text{.}\) Similarly, if \(C^2=D\text{,}\) then \(B^2=A\text{,}\) where \(B=PCP^{-1}\text{.}\)</p>
<p id="p-1100">As an example, the matrix \(A=\begin{bmatrix}0\amp -2\\ 1 \amp 3 \end{bmatrix}\text{,}\) satisfies \(D=P^{-1}AP\text{,}\) where \(D=\begin{bmatrix}1\amp 0\\ 0\amp 2 \end{bmatrix}\text{,}\) and \(P=\begin{bmatrix}2\amp 1\\ -1\amp -1 \end{bmatrix}\text{.}\) Since \(C=\begin{bmatrix}1\amp 0\\ 0\amp \sqrt{2} \end{bmatrix}\) is a square-root of \(D\text{,}\) \(B=PCP^{-1}=\begin{bmatrix}2-\sqrt{2}\amp 2-2\sqrt{2}\\ -1+\sqrt{2}\amp -1+2\sqrt{2} \end{bmatrix}\) is a square-root of \(A\text{,}\) as you can easily check.</p>
<p id="p-1101">So when exactly does a diagonal matrix \(D\) have a square-root? Clearly, it is sufficient that \(d_i\geq 0\) for all \(i\text{,}\) as in the example above. Interestingly, this is not a necessary condition! Indeed, consider the following example:</p>
<p id="p-1102">\(\begin{bmatrix}-1\amp 0\\ 0\amp -1 \end{bmatrix} =\begin{bmatrix}0\amp -1\\ 1\amp 0 \end{bmatrix} ^2\text{.}\)</p></article></div></article><article class="paragraphs" id="paragraphs-232"><h5 class="heading"><span class="title">Properties of similarity.</span></h5>
<p id="p-1103">Before investigating the question of when a matrix is diagonalizable, we record a few more properties illustrating the close connection between similar matrices.</p>
<article class="theorem theorem-like" id="th_similarity"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.7.7</span><span class="period">.</span><span class="space"> </span><span class="title">Properties of similarity.</span>
</h6>
<p id="p-1104">Suppose \(A\) is similar to \(B\text{:}\) i.e., there is an invertible matrix \(P\) such that \(B=P^{-1}AP\text{.}\) Then:</p>
<ol class="decimal">
<li id="li-258"><p id="p-1105">\(B\) is similar to \(A\text{.}\) (\alert{Similarity is symmetric}.)</p></li>
<li id="li-259"><p id="p-1106">\(A\) and \(B\) have the same trace and determinant.</p></li>
<li id="li-260"><p id="p-1107">\(A\) and \(B\) have the same rank.</p></li>
<li id="li-261"><p id="p-1108">\(A\) and \(B\) have the same characteristic polynomial.</p></li>
<li id="li-262"><p id="p-1109">\(A\) and \(B\) have the same eigenvalues.</p></li>
<li id="li-263"><p id="p-1110">Given any \(\lambda\in\R\text{,}\) let \(W_\lambda\) be the corresponding eigenspace for \(A\text{,}\) and \(W_\lambda'\) the corresponding eigenspace for \(B\text{.}\) Then \(\dim W_{\lambda}=\dim W_{\lambda}'\text{.}\)</p></li>
</ol></article><p id="p-1111">The proof of (d) follows. I leave the rest as an exercise.</p></article><article class="hiddenproof" id="proof-50"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-50"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-50"><article class="hiddenproof"><p id="p-1112">By definition we have \(B=P^{-1}AP\) for some matrix \(P\text{.}\) We wish to show the characteristic polynomials \(p_A(t)\) and \(p_B(t)\) of the two matrices are equal. Compute:</p>
<div class="displaymath">
\begin{align*}
p_B(t)\amp =\amp \det(tI-B)\\
\amp =\amp \det(tI-P^{-1}AP)\\
\amp =\amp \det(tP^{-1}IP-P^{-1}AP) \ \text{ (since \(P^{-1}IP=I\))}\\
\amp =\amp \det(P^{-1}tIP-P^{-1}AP) \ \text{ (\(t\) behaves as scalar) }\\
\amp =\amp \det(P^{-1}(tI-A)P)\\
\amp =\amp \det(P^{-1})\det(tI-A)\det(P)\\
\amp =\amp (\det(P))^{-1}\det(P)\det(tI-A)\\
\amp =\amp \det(tI-A)=p_A(t)\text{.}
\end{align*}
</div></article></div>
<article class="paragraphs" id="paragraphs-233"><h5 class="heading"><span class="title">The true meaning of similarity.</span></h5>
<p id="p-1113">Hopefully <a class="xref" data-knowl="./knowl/th_conjugation.html" title="Theorem 4.7.3: Properties of conjugation">Theorems 4.7.3</a> and <a class="xref" data-knowl="./knowl/th_similarity.html" title="Theorem 4.7.7: Properties of similarity">Theorem 4.7.7</a> convince you that similar matrices (in the linear algebraic sence) are truly similar (in the usual sense).</p>
<p id="p-1114">There is, however, a deeper explanation for this. Namely, if \(A\) and \(A'\) are similar, then they are simply two different matrix representations of a common linear transformation!</p>
<p id="p-1115">In more detail: suppose we have \(A'=P^{-1}AP\text{.}\)</p>
<ul class="disc">
<li id="li-264"><p id="p-1116">Let \(B\) be the standard basis of \(\R^n\text{,}\) and let \(B'\) be the basis of \(\R^n\) obtained by taking the columns of the invertible matrix \(P\text{.}\) Finally, let \(T=T_A\) be the matrix transformation associated to \(A\text{.}\)</p></li>
<li id="li-265"><p id="p-1117">Then \(A=[T]_B\text{,}\) \(P=\underset{B'\rightarrow B}{P}\text{,}\) and \(P^{-1}=\underset{B\rightarrow B'}{P}\text{.}\)</p></li>
<li id="li-266">
<p id="p-1118">From the change of basis formula it follows that</p>
<div class="displaymath">
\begin{equation*}
A'=P^{-1}AP=\underset{B\rightarrow B'}{P}[T]_B\underset{B'\rightarrow B}{P}=[T]_{B'}
\end{equation*}
</div>
</li>
</ul>
<p id="p-1119">In other words to say \(A\) and \(A'\) are similar is simply to say that they are different matrix representations of the same overlying linear transformation \(T\) (see Holy Commutative Tent of Linear Algebra on next slide). All their shared properties (same eigenvalues, same determinant, same trace, etc.) are simply the properties they inherit from this one overlying \(T\text{,}\) of which they are but earthly shadows.</p>
<p id="p-1120">There is one true \(T\text{!}\)</p></article><p id="p-1121"><div class="image-box" style="width: 75%; margin-left: 12.5%; margin-right: 12.5%;"><img src="images/HolyCommutativeTent.png" style="width: 100%; height: auto;" alt=""></div></p>
<p id="p-1122">The previous theorem allows us to extend some of our matrix definitions to linear transformations.</p>
<article class="definition definition-like" id="definition-48"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.7.8</span><span class="period">.</span>
</h6>
<p id="p-1123">Let \(T\colon V\rightarrow V\) be linear, let \(B\) be <dfn class="terminology">any basis</dfn> of \(V\text{,}\) and let \(A=[T]_B\text{.}\) We define the <dfn class="terminology">characteristic polynomial</dfn> of \(T\) to be \(p(t)=\det(tI-A)\text{.}\)</p></article><article class="definition definition-like" id="definition-49"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">4.7.9</span><span class="period">.</span>
</h6>
<p id="p-1124">A linear transformation \(T\colon V\rightarrow V\) is <dfn class="terminology">diagonalizable</dfn> if there exists a basis \(B\) of \(V\) for which \([T]_B\) is a diagonal matrix.</p></article><article class="paragraphs" id="paragraphs-234"><h5 class="heading"><span class="title">Diagonalizability and eigenvectors.</span></h5>
<p id="p-1125">At last we relate the property of being diagonalizable with the notion of eigenvectors. In the process we make clear what we mean when we say \(T\) is diagonalizable if and only if it has “enough” linearly independent eigenvectors.</p>
<article class="theorem theorem-like" id="th_diagonalizability"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.7.10</span><span class="period">.</span><span class="space"> </span><span class="title">Diagonalizability theorem.</span>
</h6>
<p id="p-1126">Let \(T\colon V\rightarrow V\) be linear, \(\dim(V)=n\text{.}\)</p>
<ol class="decimal">
<li id="li-267"><p id="p-1127">(\alert{Qualitative}) Given basis \(B\) of \(V\text{,}\) the matrix \([T]_B\) is diagonal if and only if \(B\) consists of eigenvectors of \(T\text{.}\) Thus \(T\) is diagonalizable if and only if there is a basis of \(V\) consisting of eigenvectors of \(T\text{.}\)</p></li>
<li id="li-268"><p id="p-1128">(\alert{Quantitative}) Let \(\lambda_1, \lambda_2, \dots, \lambda_r\) be the <em class="emphasis">distinct</em> eigenvalues of \(T\text{,}\) let \(W_{\lambda_j}\) be the corresponding eigenspaces, and let \(n_j=\dim W_{\lambda_j}\text{.}\) Then \(T \text{ is diagonalizable } \Longleftrightarrow n_1+n_2+\cdots +n_r=n\text{.}\)</p></li>
</ol></article><article class="hiddenproof" id="proof-51"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-51"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-51"><article class="hiddenproof"><p id="p-1129">The first statement was a homework exercise. The proof of the second statement is within our means, but somewhat lengthy. I will sketch its proof elsewhere. For now it is more important to understand how to use the result.</p></article></div></article><article class="paragraphs" id="paragraphs-235"><h5 class="heading"><span class="title">Deciding whether $T\colon V\rightarrow V$ is diagonalizable.</span></h5>
<p id="p-1130">Suppose \(\dim(V)=n\text{.}\)</p>
<ol class="decimal">
<li id="li-269"><p id="p-1131">Pick a basis \(B\) of \(V\text{.}\) Set \(A=[T]_B\)</p></li>
<li id="li-270"><p id="p-1132">Find the distinct eigenvalues, \(\lambda_1,\dots, \lambda_r\text{,}\) of \(A\text{,}\) let \(W_{\lambda_i}\) be the corresponding eigenspaces, and let \(n_i=\dim W_{\lambda_i}\text{.}\)</p></li>
<li id="li-271"><p id="p-1133">\(A\) (and thus \(T\)) is diagonalizable if and only if \(n_1+n_2+\cdots +n_r=n\text{.}\)</p></li>
<li id="li-272"><p id="p-1134">If the above equality is true, compute bases for each \(W_{\lambda_i}\) in \(\R^n\text{.}\)</p></li>
<li id="li-273"><p id="p-1135">Lift all vectors from all these bases back to \(V\) using \([\hspace{5pt}]_B\text{.}\) This is a new basis \(B'\) of \(V\) consisting of eigenvectors of \(T\text{.}\)</p></li>
<li id="li-274"><p id="p-1136">\([T]_{B'}=D\) is diagonal.</p></li>
</ol></article><article class="paragraphs" id="paragraphs-236"><h5 class="heading"><span class="title">Deciding whether $A_{n\times n}$ is diagonalizable.</span></h5>
<ol class="decimal">
<li id="li-275"><p id="p-1137">Find the distinct eigenvalues, \(\lambda_1,\dots, \lambda_r\text{,}\) of \(A\text{,}\) let \(W_{\lambda_i}\) be the corresponding eigenspaces, and let \(n_i=\dim W_{\lambda_i}\text{.}\)</p></li>
<li id="li-276"><p id="p-1138">\(A\) is diagonalizable if and only if \(n_1+n_2+\cdots +n_r=n\text{.}\)</p></li>
<li id="li-277"><p id="p-1139">If the above equality is true, compute bases \(B_j\) for each \(W_{\lambda_j}\text{.}\)</p></li>
<li id="li-278"><p id="p-1140">Place all the vectors from the bases \(B_j\) as columns of a matrix \(P\text{.}\) As these eigenvectors are linearly independent, \(P\) is invertible.</p></li>
<li id="li-279"><p id="p-1141">The matrix \(D=P^{-1}AP\) is diagonal. In more detail, the \(j\)-th diagonal entry of \(D\) is the eigenvalue associated to the eigenvector in the \(j\)-th column of \(P\text{.}\) This means the first \(n_1\) diagonal entries of \(D\) are equal to \(\lambda_1\text{,}\) the next \(n_2\) entries are equal to \(\lambda_2\text{,}\) etc.</p></li>
</ol></article><article class="paragraphs" id="paragraphs-237"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1142">Take \(A=\begin{bmatrix}1\amp 1\\ 0\amp 1 \end{bmatrix}\text{.}\) Earlier I claimed that this matrix is not diagonalizable. Let's see why.</p>
<p id="p-1143">The characteristic polynomial of \(A\) is \(p(t)=(t-1)^2\text{.}\) Thus \(\lambda=1\) is the only eigenvalue of \(A\text{.}\)</p>
<p id="p-1144">We have \(W_1=\NS(I-A)=\NS\begin{bmatrix}0\amp -1\\ 0\amp 0 \end{bmatrix}\text{.}\) We see clearly that \(\rank(I-A)=1\text{,}\) and hence \(\dim W_1=\dim\NS(I-A)=2-1=1\text{.}\)</p>
<p id="p-1145">Since \(W_1\) is the only eigenspace, and since \(\dim W_1=1\ne 2\text{,}\) we conclude \(A\) is not diagonalizable.</p></article><article class="paragraphs" id="paragraphs-238"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1146">Let \(A=\begin{bmatrix}14 \amp 21 \amp 3 \amp -39 \\ 12 \amp 25 \amp 3 \amp -41 \\ 12 \amp 24 \amp 5 \amp -42 \\ 12 \amp 22 \amp 3 \amp -38 \end{bmatrix}\text{.}\)</p>
<p id="p-1147">The characteristic polynomial of \(A\) is \(p(t)=x^4 - 6x^3 + 9x^2 + 4x - 12\text{.}\) (This is not obvious, but would be a pain to compute in detail. You may take this for granted.)</p>
<p id="p-1148">Our usual factoring tricks allow us to factor this as \(p(x)=(x-2)^2(x+1)(x-3)\text{.}\)</p>
<p id="p-1149">The eigenspaces are \(W_2=\NS(2I-A), W_{-1}=\NS(-I-A)\text{,}\) and \(W_3=(3I-A)\text{.}\) I'll leave it to you to verify that they have bases \(B=\{ (3,2,0,2), (1,1,2,1)\}\text{,}\) \(B'=\{(1,1,1,1)\}\text{,}\) and \(B''=\{(3,5,6,4)\}\text{,}\) respectively.</p>
<p id="p-1150">It follows that the dimensions of the eigenspaces are 2, 1, and 1, respectively. Since \(2+1+1=4\text{,}\) we conclude that \(A\) is diagonalizable.</p>
<p id="p-1151">In more detail, we have \(D=P^{-1}AP\text{,}\) where</p>
<div class="displaymath">
\begin{equation*}
D=\begin{bmatrix}2\amp 0\amp 0\amp 0\\ 0\amp 2\amp 0\amp 0\\ 0\amp 0\amp -1\amp 0\\ 0\amp 0\amp 0\amp 3 \end{bmatrix} , \ \ P=\begin{bmatrix}3 \amp  1 \amp  1 \amp  3 \\ 2 \amp  1 \amp  1 \amp  5 \\ 0 \amp  2 \amp  1 \amp  6 \\ 2 \amp  1 \amp  1 \amp  4 \end{bmatrix}
\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-239"><h5 class="heading"><span class="title">Geometric and algebraic multiplicity.</span></h5>
<p id="p-1152">Take \(A\) (or \(T\)) and suppose the characteristic polynomial \(p(t)\) factors as</p>
<div class="displaymath">
\begin{equation*}
p(t)=(t-\lambda_1)^{n_1}(t-\lambda_2)^{n_2}\cdots (t-\lambda_r)^{n_r}\text{,}
\end{equation*}
</div>
<p class="continuation">where the \(\lambda_i\) are the <em class="emphasis">distinct</em> eigenvalues of \(A\) (or \(T\)). It turns out that the exponent \(n_i\text{,}\) called the <dfn class="terminology">algebraic multiplicity</dfn> of the eigenvalue \(\lambda_i\text{,}\) is an upper bound on \(m_i=\dim W_{\lambda_i}\text{,}\) called the <dfn class="terminology">geometric multiplicity</dfn>.</p>
<article class="theorem theorem-like" id="theorem-49"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.7.11</span><span class="period">.</span><span class="space"> </span><span class="title">Algebraic and geometric multiplicity theorem.</span>
</h6>
<p id="p-1153">Let \(A\) (or \(T\)) have characterisitc polynomial</p>
<div class="displaymath">
\begin{equation*}
p(t)=(t-\lambda_1)^{n_1}(t-\lambda_2)^{n_2}\cdots (t-\lambda_r)^{n_r}\text{,}
\end{equation*}
</div>
<p class="continuation">where the \(\lambda_i\) are the <em class="emphasis">distinct</em> eigenvalues of \(A\) (or \(T\)). Then</p>
<div class="displaymath">
\begin{equation*}
\dim W_{\lambda_i}\leq n_i:
\end{equation*}
</div>
<p class="continuation">i.e., the geometric multiplicity is less than or equal to the algebraic multiplicity.</p></article></article><article class="paragraphs" id="paragraphs-240"><h5 class="heading"><span class="title">Linear independence and eigenvectors.</span></h5>
<p id="p-1154">The following result is used to prove the diagonalizability theorem (<a class="xref" data-knowl="./knowl/th_diagonalizability.html" title="Theorem 4.7.10: Diagonalizability theorem">Theorem 4.7.10</a>), but is also very useful in its own right.</p>
<article class="theorem theorem-like" id="th_independentteigenvectors"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.7.12</span><span class="period">.</span>
</h6>
<p id="p-1155">Let \(T\colon V\rightarrow V\) be a linear transformation, and let \(S=\{\boldv_1,\dots, \boldv_r\}\) be a set of eigenvectors of \(T\) with \(T\boldv_i=\lambda_i\boldv_i\text{.}\)</p>
<p id="p-1156">If the \(\lambda_i\) are all distinct, then \(S\) is linearly independent.</p></article><article class="hiddenproof" id="proof-52"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-52"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-52"><article class="hiddenproof"><p id="p-1157">Proved elsewhere.</p></article></div>
<article class="corollary theorem-like" id="corollary-10"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">4.7.13</span><span class="period">.</span>
</h6>
<p id="p-1158">Let \(T\colon V\rightarrow V\) be a linear transformation, and suppose \(\dim V=n\text{.}\)</p>
<p id="p-1159">If \(T\) has \(n\) \alert{distinct} eigenvalues, then \(T\) is diagonalizable.</p></article><article class="hiddenproof" id="proof-53"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-53"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-53"><article class="hiddenproof"><p id="p-1160">Let \(\boldv_1, \boldv_2,\dots, \boldv_n\) be eigenvectors corresponding to these \(n\) distinct eigenvalues. The theorem tells us they form a linearly independent set. Since \(\dim V=n\text{,}\) they form a basis for \(V\) by the dimension theorem compendium. Since \(T\) has a basis of eigenvectors, it is diagonalizable.</p></article></div></article><p id="p-1161"><a class="xref" data-knowl="./knowl/th_independentteigenvectors.html" title="Theorem 4.7.12">Theorem 4.7.12</a> makes no assumption about the dimension of \(V\text{.}\) It can thus be applied to interesting infinite-dimensional examples.</p>
<article class="example example-like" id="example-14"><a data-knowl="" class="id-ref example-knowl original" data-refid="hk-example-14"><h6 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">4.7.14</span><span class="period">.</span>
</h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-example-14"><article class="example example-like"><p id="p-1162">Let \(V=C^\infty(\R)\text{,}\) and let \(T\colon V\rightarrow V\) be defined as \(T(f)=f'\text{.}\)</p>
<p id="p-1163">Let \(f_i(x)=e^{k_ix}\text{,}\) where the \(k_i\) are all distinct constants. I claim \(S=\{f_1,f_2,\dots , f_r\}\) is linearly independent.</p>
<p id="p-1164">Indeed, each \(f_i\) is an eigenvector of \(T\text{,}\) since \(T(f_i)=(e^{k_ix})'=k_ie^{k_ix}=k_if_i\text{.}\)</p>
<p id="p-1165">Since the \(k_i\)'s are all distinct, it follows that the \(f_i\) are eigenvectors with distinct eigenvalues, hence linearly independent!</p>
<p id="p-1166">Note: try proving that \(S\) is linearly independent using the the Wronskian! You get a very interesting determinant computation.</p></article></div>
<article class="paragraphs" id="paragraphs-241"><h5 class="heading"><span class="title">Final extension of the invertibility theorem.</span></h5>
<p id="p-1167">Lastly, we can add one final statement to the invertibility theorem: \(A\) is invertible if and only if \(0\) is not an eigenvalue of \(A\text{.}\)</p>
<p id="p-1168">Indeed \(0\) is an eigenvalue of \(A\) if and only if \(p(0)=\det(0I-A)=\det(-A)=0\) if and only if \(\det A=0\text{,}\) since \(\det(-A)=(-1)^n\det A\text{.}\)</p>
<p id="p-1169">Since \(\det A=0\) if and only if \(A\) is not invertible, we conclude that \(0\) is an eigenvalue of \(A\) if and only if \(A\) is not invertible.</p>
<p id="p-1170">You find the final version of the invertibility theorem on the next slide.</p></article><article class="theorem theorem-like" id="theorem-51"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">4.7.15</span><span class="period">.</span><span class="space"> </span><span class="title">Invertibility theorem (final version).</span>
</h6>
<p id="p-1171">Let \(A\) be \(n\times n\text{.}\) The following statements are equivalent.</p>
<ol class="decimal">
<li id="li-280"><p id="p-1172">\(A\) is invertible.</p></li>
<li id="li-281"><p id="p-1173">\(A\boldx=\boldzero\) has a unique solution (the trivial one).</p></li>
<li id="li-282"><p id="p-1174">\(A\) is row equivalent to \(I_n\text{,}\) the \(n\times n\) identity matrix.</p></li>
<li id="li-283"><p id="p-1175">\(A\) is a product of elementary matrices.</p></li>
<li id="li-284"><p id="p-1176">\(A\boldx=\boldb\) has a solution for every \(n\times 1\) column vector \(\boldb\text{.}\)</p></li>
<li id="li-285"><p id="p-1177">\(A\boldx=\boldb\) has a <em class="emphasis">unique</em> solution for every \(n\times 1\) column vector \(\boldb\text{.}\)</p></li>
<li id="li-286"><p id="p-1178">\(\det(A)\ne 0\text{.}\)</p></li>
<li id="li-287"><p id="p-1179">\(\NS(A)=\{\boldzero\}\text{.}\)</p></li>
<li id="li-288"><p id="p-1180">\(\nullity(A)=0\text{.}\)</p></li>
<li id="li-289"><p id="p-1181">\(\rank(A)=n\text{.}\)</p></li>
<li id="li-290"><p id="p-1182">\(\CS(A)=\R^n\text{.}\)</p></li>
<li id="li-291"><p id="p-1183">\(\RS(A)=\R^n\text{.}\)</p></li>
<li id="li-292"><p id="p-1184">The columns of \(A\) are linearly independent (or span \(\R^n\text{,}\) or are a basis of \(\R^n\)).</p></li>
<li id="li-293"><p id="p-1185">The rows of \(A\) are linearly independent (or span \(\R^n\text{,}\) or are a basis of \(\R^n\)).</p></li>
<li id="li-294"><p id="p-1186">\(A\) does not have 0 as an eigenvalue.</p></li>
</ol></article></section></div></main>
</div>
</body>
</html>
