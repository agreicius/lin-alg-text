<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2021-05-24T17:10:38-05:00       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Orthogonality, Gram-Schmidt, orthogonal projection</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']]
    },
    asciimath2jax: {
        ignoreClass: ".*",
        processClass: "has_am"
    },
    jax: ["input/AsciiMath"],
    extensions: ["asciimath2jax.js"],
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://pretextbook.org/js/lib/mathjaxknowl.js", "AMScd.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/0.13/pretext_add_on.js"></script><script xmlns:svg="http://www.w3.org/2000/svg" src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script xmlns:svg="http://www.w3.org/2000/svg">sagecellEvalName='Evaluate (Sage)';
</script><link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/pretext_add_on.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/banner_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/toc_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/knowls_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/style_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/colors_default.css" rel="stylesheet" type="text/css">
<link xmlns:svg="http://www.w3.org/2000/svg" href="https://pretextbook.org/css/0.31/setcolors.css" rel="stylesheet" type="text/css">
<!-- 2019-10-12: Temporary - CSS file for experiments with styling --><link xmlns:svg="http://www.w3.org/2000/svg" href="developer.css" rel="stylesheet" type="text/css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div xmlns:svg="http://www.w3.org/2000/svg" id="latex-macros" class="hidden-content" style="display:none">\(\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\F}{{\mathbb F}}
\newcommand{\HH}{{\mathbb H}}
\newcommand{\compose}{\circ}
\newcommand{\bolda}{{\mathbf a}}
\newcommand{\boldb}{{\mathbf b}}
\newcommand{\boldc}{{\mathbf c}}
\newcommand{\boldd}{{\mathbf d}}
\newcommand{\bolde}{{\mathbf e}}
\newcommand{\boldi}{{\mathbf i}}
\newcommand{\boldj}{{\mathbf j}}
\newcommand{\boldk}{{\mathbf k}}
\newcommand{\boldn}{{\mathbf n}}
\newcommand{\boldp}{{\mathbf p}}
\newcommand{\boldq}{{\mathbf q}}
\newcommand{\boldr}{{\mathbf r}}
\newcommand{\boldsymbol}{{\mathbf s}}
\newcommand{\boldt}{{\mathbf t}}
\newcommand{\boldu}{{\mathbf u}}
\newcommand{\boldv}{{\mathbf v}}
\newcommand{\boldw}{{\mathbf w}}
\newcommand{\boldx}{{\mathbf x}}
\newcommand{\boldy}{{\mathbf y}}
\newcommand{\boldz}{{\mathbf z}}
\newcommand{\boldzero}{{\mathbf 0}}
\newcommand{\boldmod}{\boldsymbol{ \bmod }}
\newcommand{\boldT}{{\mathbf T}}
\newcommand{\boldN}{{\mathbf N}}
\newcommand{\boldB}{{\mathbf B}}
\newcommand{\boldF}{{\mathbf F}}
\newcommand{\boldS}{{\mathbf S}}
\newcommand{\boldG}{{\mathbf G}}
\newcommand{\boldK}{{\mathbf K}}
\newcommand{\boldL}{{\mathbf L}}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\NS}{null}
\DeclareMathOperator{\RS}{row}
\DeclareMathOperator{\CS}{col}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Aff}{Aff}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\Lt}{Lt}
\DeclareMathOperator{\Lc}{Lc}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\Frob}{Frob}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\diver}{div}
\DeclareMathOperator{\flux}{flux}
\def\Gal{\operatorname{Gal}}
\def\ord{\operatorname{ord}}
\def\ML{\operatorname{M}}
\def\GL{\operatorname{GL}}
\def\PGL{\operatorname{PGL}}
\def\SL{\operatorname{SL}}
\def\PSL{\operatorname{PSL}}
\def\GSp{\operatorname{GSp}}
\def\PGSp{\operatorname{PGSp}}
\def\Sp{\operatorname{Sp}}
\def\PSp{\operatorname{PSp}}
\def\Aut{\operatorname{Aut}}
\def\Inn{\operatorname{Inn}}
\def\Hom{\operatorname{Hom}}
\def\End{\operatorname{End}}
\def\ch{\operatorname{char}}
\def\Zp{\Z/p\Z}
\def\Zm{\Z/m\Z}
\def\Zn{\Z/n\Z}
\def\Fp{\F_p}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\injects}{\hookrightarrow}
\newcommand{\bijects}{\leftrightarrow}
\newcommand{\isomto}{\overset{\sim}{\rightarrow}}
\newcommand{\floor}[1]{\lfloor#1\rfloor}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\mclass}[2][m]{[#2]_{#1}}
\newcommand{\val}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\valuation}[2][]{\left\lvert #2\right\rvert_{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\anpoly}{a_nx^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\anmonic}{x^n+a_{n-1}x^{n-1}\cdots +a_1x+a_0}
\newcommand{\bmpoly}{b_mx^m+b_{m-1}x^{m-1}\cdots +b_1x+b_0}
\newcommand{\pder}[2]{\frac{\partial#1}{\partial#2}}
\renewcommand{\c}{\cancel}
\newcommand{\normalin}{\trianglelefteq}
\newcommand{\angvec}[1]{\langle #1\rangle}
\newcommand{\varpoly}[2]{#1_{#2}x^{#2}+#1_{#2-1}x^{#2-1}\cdots +#1_1x+#1_0}
\newcommand{\varpower}[1][a]{#1_0+#1_1x+#1_1x^2+\cdots}
\newcommand{\limasto}[2][x]{\lim_{#1\rightarrow #2}}
\def\ntoinfty{\lim_{n\rightarrow\infty}}
\def\xtoinfty{\lim_{x\rightarrow\infty}}
\def\ii{\item}
\def\bb{\begin{enumerate}}
\def\ee{\end{enumerate}}
\def\ds{\displaystyle}
\def\p{\partial}
\newcommand{\abcdmatrix}[4]{\begin{bmatrix}#1\amp #2\\ #3\amp #4 \end{bmatrix}
}
\newenvironment{amatrix}[1][ccc|c]{\left[\begin{array}{#1}}{\end{array}\right]}
\newenvironment{linsys}[2][m]{
\begin{array}[#1]{@{}*{#2}{rc}r@{}}
}{
\end{array}}
\newcommand{\eqsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\numeqsys}{\begin{array}{rrcrcrcr}
e_1:\amp  a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp b_1\\
e_2: \amp a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp b_2\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
e_m: \amp a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp b_m
\end{array}
}
\newcommand{\homsys}{\begin{array}{rcrcrcr}
a_{11}x_{1}\amp +\amp a_{12}x_{2}\amp +\cdots+\amp  a_{1n}x_{n}\amp =\amp 0\\
a_{21}x_{1}\amp +\amp a_{22}x_{2}\amp +\cdots+\amp a_{2n}x_{n}\amp =\amp 0\\
\amp \vdots\amp   \amp \vdots \amp  \amp \vdots \amp  \vdots\\
a_{m1}x_{1}\amp +\amp a_{m2}x_{2}\amp +\cdots +\amp a_{mn}x_{n}\amp =\amp 0
\end{array}
}
\newcommand{\vareqsys}[4]{
\begin{array}{ccccccc}
#3_{11}x_{1}\amp +\amp #3_{12}x_{2}\amp +\cdots+\amp  #3_{1#2}x_{#2}\amp =\amp #4_1\\
#3_{21}x_{1}\amp +\amp #3_{22}x_{2}\amp +\cdots+\amp #3_{2#2}x_{#2}\amp =\amp #4_2\\
\vdots \amp \amp \vdots \amp  \amp \vdots \amp =\amp  \vdots\\
#3_{#1 1}x_{1}\amp +\amp #3_{#1 2}x_{2}\amp +\cdots +\amp #3_{#1 #2}x_{#2}\amp =\amp #4_{#1}
\end{array}
}
\newcommand{\genmatrix}[1][a]{
\begin{bmatrix}
#1_{11} \amp  #1_{12} \amp  \cdots \amp  #1_{1n} \\
#1_{21} \amp  #1_{22} \amp  \cdots \amp  #1_{2n} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#1_{m1} \amp  #1_{m2} \amp  \cdots \amp  #1_{mn}
\end{bmatrix}
}
\newcommand{\varmatrix}[3]{
\begin{bmatrix}
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}
\end{bmatrix}
}
\newcommand{\augmatrix}{
\begin{amatrix}[cccc|c]
a_{11} \amp  a_{12} \amp  \cdots \amp  a_{1n} \amp b_{1}\\
a_{21} \amp  a_{22} \amp  \cdots \amp  a_{2n} \amp b_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
a_{m1} \amp  a_{m2} \amp  \cdots \amp  a_{mn}\amp b_{m}
\end{amatrix}
}
\newcommand{\varaugmatrix}[4]{
\begin{amatrix}[cccc|c]
#3_{11} \amp  #3_{12} \amp  \cdots \amp  #3_{1#2} \amp #4_{1}\\
#3_{21} \amp  #3_{22} \amp  \cdots \amp  #3_{2#2} \amp #4_{2}\\
\vdots  \amp  \vdots  \amp  \ddots \amp  \vdots  \amp \vdots\\
#3_{#1 1} \amp  #3_{#1 2} \amp  \cdots \amp  #3_{#1 #2}\amp #4_{#1}
\end{amatrix}
}
\newcommand{\spaceforemptycolumn}{\makebox[\wd\boxofmathplus]{\ }}
\newcommand{\grstep}[2][\relax]{
\mathrel{
\hspace{\grsteplength}\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
\begin{subarray}{l} #1 \end{subarray}}\hspace{\grsteplength}}}
\newcommand{\repeatedgrstep}[2][\relax]{\hspace{-\grsteplength}\grstep[#1]{#2}}
\newcommand{\swap}{\leftrightarrow}
\newcommand{\deter}[1]{ \mathchoice{\left|#1\right|}{|#1|}{|#1|}{|#1|} }
\newcommand{\generalmatrix}[3]{
\left(
\begin{array}{cccc}
#1_{1,1}  \amp #1_{1,2}  \amp \ldots  \amp #1_{1,#2}  \\
#1_{2,1}  \amp #1_{2,2}  \amp \ldots  \amp #1_{2,#2}  \\
\amp \vdots                         \\
#1_{#3,1} \amp #1_{#3,2} \amp \ldots  \amp #1_{#3,#2}
\end{array}
\right)  }
\newcommand{\colvec}[2][c]{\begin{bmatrix}[#1] #2 \end{bmatrix}}
\newcommand{\rowvec}[1]{\setlength{\arraycolsep}{3pt}\begin{bmatrix} #1 \end{bmatrix}}
\DeclareMathOperator{\trace}{tr}
\newcommand{\isomorphicto}{\cong}
\newcommand{\rangespace}[1]{ \mathscr{R}(#1) }
\newcommand{\nullspace}[1]{ \mathscr{N}(#1) }
\newcommand{\genrangespace}[1]{ \mathscr{R}_\infty(#1) }
\newcommand{\gennullspace}[1]{ \mathscr{N}_\infty(#1) }
\newcommand{\zero}{ \vec{0} }
\newcommand{\polyspace}{\mathcal{P}}
\newcommand{\matspace}{\mathcal{M}}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\adjoint}{adj}
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\definend}[1]{\emph{#1}}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\map}[3]{\mbox{$#1\colon #2\to #3$}}
\newcommand{\mapsunder}[1]{\stackrel{#1}{\longmapsto}}
\newcommand{\mapsvia}[1]{\xrightarrow{#1}}
\newcommand{\xmapsunder}[1]{\mapsunder{#1}}
\newcommand{\composed}[2]{#1\mathbin{\circ} #2}
\DeclareMathOperator{\identity}{id}
\newcommand{\restrictionmap}[2]{{#1}\mathpunct\upharpoonright\hbox{}_{#2}}
\renewcommand{\emptyset}{\varnothing}
\newcommand{\setspacing}{0.1em}
\newcommand{\set}[1]{\mbox{$\{\hspace{\setspacing}#1\hspace{\setspacing}\}$}}
\newcommand{\suchthat}{\mid}
\newcommand{\union}{\cup}
\newcommand{\intersection}{\cap}
\newcommand{\sequence}[1]{ \langle#1\rangle }
\newcommand{\interval}[2]{#1\,\ldots\, #2}
\newcommand{\setinterval}[2]{\mbox{$\{\interval{#1}{#2}\}$}}
\newcommand{\openinterval}[2]{(\interval{#1}{#2})}
\newcommand{\closedinterval}[2]{[\interval{#1}{#2}]}
\newcommand{\clopinterval}[2]{[\interval{#1}{#2})}
\newcommand{\opclinterval}[2]{(\interval{#1}{#2}]}
\newcommand{\isimpliedby}{\Longleftarrow}
\newcommand{\Sage}{\textit{Sage}}
\newcommand{\Maple}{\textit{Maple}}
\newcommand{\cat}[2]{#1\!\mathbin{\raise.6ex\hbox{\left( {}^\frown \right)}}\!#2}
\newcommand{\alignedvdots}[1][10pt]{\mskip2.5mu\makebox[.5\equalsignwd][r]{}
\smash{\vdots}}
\newcommand{\stdbasis}{{\cal E}}
\newcommand{\basis}[2]{\sequence{\vec{#1}_1,\ldots,\vec{#1}_{#2}}}
\newcommand{\rowspace}[1]{ \mathop{{\mbox{Rowspace}}}(#1) }
\newcommand{\colspace}[1]{ \mathop{{\mbox{Columnspace}}}(#1) }
\newcommand{\linmaps}[2]{ \mathop{{\cal L}}(#1,#2) }
\newcommand{\lincombo}[2]{
#1_1#2_1+#1_2#2_2+\cdots +#1_n#2_n}
\newcommand{\rep}[2]{ { Rep}_{#2}(#1) }
\newcommand{\wrt}[1]{{\mbox{\scriptsize \textit{wrt}\hspace{.25em}\left( #1 \right)} }}
\newcommand{\trans}[1]{ {{#1}^{\mathsf{T}}} }
\newcommand{\proj}[2]{\mbox{proj}_{#2}({#1}) }
\newcommand{\spanof}[1]{\relax [#1\relax ]}
\newcommand{\directsum}{\oplus}
\DeclareMathOperator{\dist}{dist}
\newcommand{\nbyn}[1]{#1 \! \times \! #1 }
\newcommand{\nbym}[2]{#1 \! \times \! #2 }
\newcommand{\degs}[1]{#1^\circ\relax}
\newcommand{\votepreflist}[3]{\colvec{#1 \\ #2 \\ #3}}
\newcommand{\votinggraphic}[1]{\hspace{1.15em}\mathord{[.3in][.2in]{\includegraphics{voting.#1}}}\hspace{1.15em}}
\newcommand{\magicsquares}{\mathscr{M}}
\newcommand{\semimagicsquares}{\mathscr{H}}
\newcommand{\circuitfont}{\sffamily}
\newcommand{\digitinsq}[1]{\fbox{\left( #1 \right)} }
\newcommand{\matrixvenlarge}[1]{\vbox{
\vspace{\extramatrixvspace}
\hbox{$#1$}
\vspace{\extramatrixvspace}
}}
\def\bspace{
{\vspace{.05in}}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="book-1.html"><span class="title">Linear algebra: the theory of vector spaces</span></a></h1>
<p class="byline">Aaron Greicius</p>
</div>
</div></div>
<nav xmlns:svg="http://www.w3.org/2000/svg" id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3">
<a class="index-button toolbar-item button" href="index-1.html" title="Index">Index</a><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="ss_innerproducts.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="c_innerproductspaces.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="backmatter-1.html" title="Next">Next</a></span>
</div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="ss_innerproducts.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="c_innerproductspaces.html" title="Up">Up</a><a class="next-button button toolbar-item" href="backmatter-1.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div xmlns:svg="http://www.w3.org/2000/svg" id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter"><a href="frontmatter-1.html" data-scroll="frontmatter-1"><span class="title">Front Matter</span></a></li>
<li class="link">
<a href="c_linear_systems.html" data-scroll="c_linear_systems"><span class="codenumber">1</span> <span class="title">Linear systems of equations</span></a><ul>
<li><a href="s_systems.html" data-scroll="s_systems">Linear systems of equations</a></li>
<li><a href="s_ge.html" data-scroll="s_ge">Gaussian elimination</a></li>
<li><a href="s_solving.html" data-scroll="s_solving">Solving linear systems</a></li>
<li><a href="ss_matrix.html" data-scroll="ss_matrix">Matrix arithmetic</a></li>
<li><a href="ss_algebraic.html" data-scroll="ss_algebraic">Algebraic properties</a></li>
<li><a href="ss_elementary.html" data-scroll="ss_elementary">Elementary matrices</a></li>
<li><a href="ss_invertible.html" data-scroll="ss_invertible">More on invertibility</a></li>
</ul>
</li>
<li class="link">
<a href="c_det.html" data-scroll="c_det"><span class="codenumber">2</span> <span class="title">The determinant</span></a><ul>
<li><a href="ss_det.html" data-scroll="ss_det">The determinant</a></li>
<li><a href="ss_rowops.html" data-scroll="ss_rowops">Row operations and the determinant</a></li>
<li><a href="ss_further.html" data-scroll="ss_further">Further properties</a></li>
</ul>
</li>
<li class="link">
<a href="c_vectorspace.html" data-scroll="c_vectorspace"><span class="codenumber">3</span> <span class="title">Vector spaces and linear transformations</span></a><ul>
<li><a href="ss_vectorspace.html" data-scroll="ss_vectorspace">Real vector spaces</a></li>
<li><a href="ss_transformation.html" data-scroll="ss_transformation">Linear transformations</a></li>
<li><a href="ss_subspace.html" data-scroll="ss_subspace">Subspaces</a></li>
<li><a href="ss_independence.html" data-scroll="ss_independence">Linear independence</a></li>
<li><a href="ss_basis.html" data-scroll="ss_basis">Bases</a></li>
<li><a href="ss_dimension.html" data-scroll="ss_dimension">Dimension</a></li>
</ul>
</li>
<li class="link">
<a href="c_transbasis.html" data-scroll="c_transbasis"><span class="codenumber">4</span> <span class="title">Linear transformations, bases, and dimension</span></a><ul>
<li><a href="ss_rank-nullity.html" data-scroll="ss_rank-nullity">Linear transformations and bases</a></li>
<li><a href="ss_rank-nullity.html" data-scroll="ss_rank-nullity">Linear transformations and bases</a></li>
<li><a href="ss_coordinate.html" data-scroll="ss_coordinate">Coordinate vectors</a></li>
<li><a href="ss_matrixreps.html" data-scroll="ss_matrixreps">Matrix representations</a></li>
<li><a href="ss_changeofbasis.html" data-scroll="ss_changeofbasis">Change of basis</a></li>
<li><a href="ss_eigenvectors.html" data-scroll="ss_eigenvectors">Eigenvectors and eigenvalues</a></li>
<li><a href="ss_diagonalization.html" data-scroll="ss_diagonalization">Diagonalization</a></li>
</ul>
</li>
<li class="link">
<a href="c_innerproductspaces.html" data-scroll="c_innerproductspaces"><span class="codenumber">5</span> <span class="title">Inner product spaces</span></a><ul>
<li><a href="ss_innerproducts.html" data-scroll="ss_innerproducts">Inner products, norms, angles</a></li>
<li><a href="ss_orthogonality.html" data-scroll="ss_orthogonality" class="active">Orthogonality, Gram-Schmidt, orthogonal projection</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter-1.html" data-scroll="backmatter-1"><span class="title">Back Matter</span></a></li>
<li class="link"><a href="appendix-defs.html" data-scroll="appendix-defs"><span class="codenumber">A</span> <span class="title">Definitions</span></a></li>
<li class="link"><a href="appendix-thms.html" data-scroll="appendix-thms"><span class="codenumber">B</span> <span class="title">Theorems</span></a></li>
<li class="link"><a href="appendix-egs.html" data-scroll="appendix-egs"><span class="codenumber">C</span> <span class="title">Examples</span></a></li>
<li class="link"><a href="references-1.html" data-scroll="references-1"><span class="title">Bibliography</span></a></li>
<li class="link"><a href="index-1.html" data-scroll="index-1"><span class="title">Index</span></a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section xmlns:svg="http://www.w3.org/2000/svg" class="section" id="ss_orthogonality"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">5.2</span> <span class="title">Orthogonality, Gram-Schmidt, orthogonal projection</span>
</h2>
<article class="paragraphs" id="paragraphs-269"><h5 class="heading"><span class="title">Orthogonal sets.</span></h5>
<article class="definition definition-like" id="definition-52"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.2.1</span><span class="period">.</span>
</h6>
<p id="p-1258">Let \((V,\langle \ , \rangle)\) be an inner product space.</p>
<p id="p-1259">A set \(S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}\) of <dfn class="terminology">nonzero vectors</dfn> is <dfn class="terminology">orthogonal</dfn> if \(\langle\boldv_i,\boldv_j\rangle=0\) for all \(i\ne j\text{:}\) i.e., the elements are pairwise orthogonal.</p>
<p id="p-1260">An orthogonal set that further satisfies \(\norm{\boldv_i}=1\) for all \(i\) is called <dfn class="terminology">orthonormal</dfn>.</p></article><article class="theorem theorem-like" id="theorem-54"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.2</span><span class="period">.</span>
</h6>
<p id="p-1261">Let \((V,\langle\ , \rangle)\) be an inner product space. If \(S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}\) is orthogonal, then \(S\) is linearly independent.</p></article><article class="hiddenproof" id="proof-57"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-57"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-57"><article class="hiddenproof"><div class="displaymath">
\begin{align*}
a_1\boldv_1 +a_2\boldv_2+\cdots +a_r\boldv_r=\boldzero\amp \Rightarrow\amp  \langle a_1\boldv_1 +a_2\boldv_2 +\cdots +a_r\boldv_r,\boldv_i\rangle=\langle\boldzero,\boldv_i\rangle\\
\amp \Rightarrow\amp  a_1\langle\boldv_1,\boldv_i\rangle +a_2\langle \boldv_2,\boldv_i\rangle +\cdots +a_r\langle\boldv_r,\boldv_i\rangle=0\\
\amp \Rightarrow\amp  a_i\langle \boldv_i,\boldv_i\rangle=0 \ \text{ (since \(\langle\boldv_j,\boldv_i\rangle= 0\) for \(j\ne i\)) }\\
\amp \Rightarrow\amp  a_i=0  \text{ (since \(\langle\boldv_i,\boldv_i\rangle\ne 0\)) }
\end{align*}
</div>
<p id="p-1262">We have shown that if \(a_1\boldv_1+a_2\boldv_2+\cdots +a_r\boldv_r=\boldzero\text{,}\) then \(a_i=0\) for all \(i\text{,}\) proving that \(S\) is linearly independent.</p></article></div></article><article class="paragraphs" id="paragraphs-270"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1263">Let \(V=C([0,2\pi])\) with standard inner product \(\langle f, g\rangle=\int_0^{2\pi} f(x)g(x) \, dx\text{.}\)</p>
<p id="p-1264">Let</p>
<div class="displaymath">
\begin{equation*}
S=\{\cos(x),\sin(x),\cos(2x),\sin(2x), \dots\}=\{\cos(nx)\colon n\in\Z_{&gt;0}\}\cup\{\sin(mx)\colon m\in\Z_{&gt;0}\}\text{.}
\end{equation*}
</div>
<p id="p-1265">Then \(S\) is orthogonal, hence linearly independent.</p>
<article class="hiddenproof" id="proof-58"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-58"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-58"><article class="hiddenproof"><p id="p-1266">Using some trig identities, one can show the following:</p>
<div class="displaymath">
\begin{align*}
\langle \cos(nx),\cos(mx)\rangle=\int_0^{2\pi}\cos(nx)\cos(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }\\
\pi\amp  \text{ if \(n=m\) }  \end{cases}\\
\langle \sin(nx),\sin(mx)\rangle=\int_0^{2\pi}\sin(nx)\sin(mx)\, dx\amp =\begin{cases} 0\amp  \text{ if \(n\ne m\) }\\
\pi\amp  \text{ if \(n=m\) }  \end{cases}\\
\langle \cos(nx),\sin(mx)\rangle=\int_0^{2\pi}\cos(nx)\sin(mx)\, dx\amp =0 \text{ for any \(n,m\) }
\end{align*}
</div></article></div>
<p id="p-1267">Orthogonality holds more generally if we replace the interval \([0,2\pi]\) with any interval of length \(L\text{,}\) and replace \(S\) with</p>
<div class="displaymath">
\begin{equation*}
\scriptsize \left\{\cos\left(\frac{2\pi x}{L}\right), \sin\left(\frac{2\pi x}{L}\right), \cos\left(2\cdot\frac{2\pi x}{L}\right),\sin\left(2\cdot\frac{2\pi x}{L}\right),\dots\right\}\text{.}
\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-271"><h5 class="heading"><span class="title">Orthogonal basis.</span></h5>
<article class="definition definition-like" id="definition-53"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.2.3</span><span class="period">.</span>
</h6>
<p id="p-1268">Let \((V,\langle \ , \rangle)\) be an inner product space, \(\dim V=n\text{.}\)</p>
<p id="p-1269">An <dfn class="terminology">orthogonal basis</dfn> is a basis \(B=\{\boldv_1,\dots,\boldv_n\}\) that is an orthogonal set; an orthogonal basis \(B\) is <dfn class="terminology">orthonormal</dfn> if \(B\) is orthonormal.</p></article><article class="theorem theorem-like" id="theorem-55"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.4</span><span class="period">.</span><span class="space"> </span><span class="title">Existence/extension of orthonormal bases.</span>
</h6>
<p id="p-1270">Let \((V,\langle \ , \rangle)\) be a finite-dimensional inner product vector space.</p>
<p id="p-1271">Then <em class="emphasis">(1)</em> \(V\) has an ortho(gonal/normal) basis. (Proof is the <em class="emphasis">Gram-Schmidt</em> process. See two slides on.)</p>
<p id="p-1272">Furthermore, <em class="emphasis">(2)</em> any ortho(gonal/normal) set can be <em class="emphasis">extended</em> to an ortho(gonal/normal) basis of \(V\text{.}\)</p></article></article><article class="theorem theorem-like" id="theorem-56"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.5</span><span class="period">.</span><span class="space"> </span><span class="title">Calculating with ortho(gonal/normal) bases.</span>
</h6>
<p id="p-1273">Let \(B=\{\boldv_1,\dots,\boldv_n\}\) be an <em class="emphasis">orthogonal basis</em> for \(V\text{.}\)</p>
<p id="p-1274">Given any \(\boldv\in V\) we have \(\boldv=a_1\boldv_1+a_2\boldb_2+\cdots +a_n\boldv_n\) where \(a_i=\frac{\langle \boldv,\boldv_i\rangle}{\langle\boldv_i,\boldv_i\rangle}\text{.}\)</p>
<p id="p-1275">Equivalently,</p>
<div class="displaymath">
\begin{equation*}
[\boldv]_B=\begin{bmatrix}\frac{\langle \boldv,\boldv_1\rangle}{\langle\boldv_1,\boldv_1\rangle}\\ \frac{\langle \boldv,\boldv_2\rangle}{\langle\boldv_2,\boldv_2\rangle}\\ \vdots \\ \frac{\langle \boldv,\boldv_n\rangle}{\langle\boldv_n,\boldv_n\rangle} \end{bmatrix}
\end{equation*}
</div>
<p id="p-1276">If \(B\) is ortho\alert{normal} then these formulas simplify to</p>
<div class="displaymath">
\begin{equation*}
a_i=\langle\boldv,\boldv_i\rangle \text{ for all \(i\) }\text{.}
\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-272"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1277">Let \(V=\R^2\) with the standard inner produce (aka the dot product).</p>
<p id="p-1278">(a) Verify that \(B'=\{\boldv_1=(\sqrt{3}/2,1/2), \boldv_2=(-1/2,\sqrt{3}/2)\}\) is an orthonormal basis.</p>
<p id="p-1279">(b) Compute \([\boldv]_{B'}\) for \(\boldv=(4,2)\text{.}\)</p>
<p id="p-1280">(c) Compute \(\underset{B\rightarrow B'}{P}\text{,}\) where \(B\) is the standard basis. \​begin{bsolution}</p>
<p id="p-1281">(a) Easily seen to be true.</p>
<p id="p-1282">(b) Since \(B'\) is orthonormal, \(\boldv=a_1\boldv_1+a_2\boldv_2\) where \(a_1=\boldv\cdot\boldv_1=2\sqrt{3}+1\) and \(a_2=\boldv\cdot\boldv_2=\sqrt{3}-2\text{.}\) Thus \([\boldv]_{B'}=\begin{bmatrix}2\sqrt{3}+1\\ \sqrt{3}-2 \end{bmatrix}\)</p>
<p id="p-1283">(c) As we have seen before, \(\underset{B'\rightarrow B}{P}=\begin{bmatrix}\sqrt{3}/2\amp -1/2\\1/2\amp \sqrt{3}/2 \end{bmatrix}\) (put elements of \(B'\) in as columns). Hence \(\underset{B\rightarrow B'}{P}=(\underset{B'\rightarrow B}{P})^{-1}=\begin{bmatrix}\sqrt{3}/2\amp 1/2\\-1/2\amp \sqrt{3}/2 \end{bmatrix}\) \end{bsolution}</p>
<article class="paragraphs" id="paragraphs-273"><h5 class="heading"><span class="title">Useful fact.</span></h5>
<p id="p-1284">If the columns of an \(n\times n\) matrix \(P\) are orthonormal, then \(P\) is invertible, and \(P^{-1}=P^T\text{.}\)</p></article><article class="paragraphs" id="paragraphs-274"><h5 class="heading"><span class="title">Proof.</span></h5>
<p id="p-1285">Let \(\boldp_j\) be the \(j\)-th column of \(P\text{.}\) By the <em class="emphasis">dot product method</em> of matrix multiplication, we have \(P^TP=[\boldp_i\cdot\boldp_j]=I_n\text{,}\) since the \(\boldp_i\) are orthonormal. This proves \(P^T=P^{-1}\text{.}\)</p></article></article><article class="paragraphs" id="paragraphs-275"><h5 class="heading"><span class="title">Gram-Schmidt Process.</span></h5>
<p id="p-1286">The proof that every finite-dimensional vector space has an orthogonal basis is actually a procedure, called the <em class="emphasis">Gram-Schmidt process</em>, for converting an arbitrary basis for an inner product space to an orthogonal basis.</p>
<article class="theorem theorem-like" id="theorem-57"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.6</span><span class="period">.</span><span class="space"> </span><span class="title">Gram-Schmidt process.</span>
</h6>
<p id="p-1287">Let \((V, \langle \ , \ \rangle)\) be an inner product space, and let \(B=\{\boldv_1, \boldv_2, \dots, \boldv_n\}\) be a basis for \(V\text{.}\) We can convert \(B\) into an orthogonal basis \(B'=\{\boldw_1, \boldw_2, \dots, \boldw_n\}\) using the following recursive procedure:</p>
<ol class="decimal">
<li id="li-299"><p id="p-1288">Set \(\boldw_1=\boldv_1\text{.}\)</p></li>
<li id="li-300">
<p id="p-1289">For \(2\leq r\leq n\) replace \(\boldv_r\) with</p>
<div class="displaymath">
\begin{equation*}
\boldw_r:=\boldv_r-\frac{\angvec{\boldv_r, \boldw_{r-1}}}{\angvec{\boldw_{r-1},\boldw_{r-1}}}\boldw_{r-1}-\frac{\angvec{\boldv_r, \boldw_{r-2}}}{\angvec{\boldw_{r-2},\boldw_{r-2}}}\boldw_{r-2}-\cdots -\frac{\angvec{\boldv_r, \boldw_{1}}}{\angvec{\boldw_{1},\boldw_{1}}}\boldw_1
\end{equation*}
</div>
</li>
</ol>
<p id="p-1290">Lastly, to further transform to an orthonormal basis, replace each \(\boldw_i\) with \(\ds \boldu_i=\frac{\boldw_i}{\norm{\boldw_i}}\text{.}\)</p></article></article><article class="paragraphs" id="paragraphs-276"><h5 class="heading"><span class="title">Orthogonal complement.</span></h5>
<article class="definition definition-like" id="definition-54"><h6 class="heading">
<span class="type">Definition</span><span class="space"> </span><span class="codenumber">5.2.7</span><span class="period">.</span>
</h6>
<p id="p-1291">. Let \((V,\langle \ , \rangle)\) be an inner product vector space, and let \(W\subseteq V\) be a finite-dimensional subspace. The <dfn class="terminology">orthogonal complement of \(W\)</dfn> is defined as</p>
<div class="displaymath">
\begin{equation*}
W^\perp:=\{\boldv\in V\colon \langle \boldv, \boldw\rangle=0 \text{ for all } \boldw\in W\}\text{.}
\end{equation*}
</div>
<p id="p-1292">In English: \(W^\perp\) is the set of vectors that are orthogonal to <dfn class="terminology">all</dfn> vectors in \(W\text{.}\)</p></article><article class="theorem theorem-like" id="theorem-58"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.8</span><span class="period">.</span><span class="space"> </span><span class="title">Orthogonal complement theorem.</span>
</h6>
<p id="p-1293">Let \((V,\langle \ , \rangle)\) be an inner product vector space, and let \(W\subseteq V\) be a subspace.</p>
<ol class="decimal">
<li id="li-301"><p id="p-1294">\(W^\perp\) is a subspace of \(V\text{.}\)</p></li>
<li id="li-302"><p id="p-1295">\(\displaystyle W\cap W^\perp=\{\boldzero\}\)</p></li>
</ol></article><article class="paragraphs" id="paragraphs-277"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1296">Let \(V=\R^3\) equipped with the dot product, and let \(W=\Span\{(1,1,1)\}\subset \R^3\text{.}\) This is the line defined by the vector \((1,1,1)\text{.}\) Then \(W^\perp\) is the set of vectors orthogonal to \((1,1,1)\text{:}\) i.e., the plane perpendicular to \((1,1,1)\text{.}\)</p></article></article><article class="paragraphs" id="paragraphs-278"><h5 class="heading"><span class="title">Geometry of fundamental spaces.</span></h5>
<p id="p-1297">The notion of orthogonal complement gives us a new way of understanding the relationship between the various fundamental spaces of a matrix.</p>
<article class="theorem theorem-like" id="theorem-59"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.9</span><span class="period">.</span>
</h6>
<p id="p-1298">Let \(A\) be \(m\times n\text{,}\) and consider \(\R^n\) and \(\R^m\) as inner product spaces with respect to the dot product. Then:</p>
<ol class="decimal">
<li id="li-303"><p id="p-1299">\(\NS(A)=\left(\RS(A)\right)^\perp\text{,}\) and thus \(\RS(A)=\left(\NS(A)\right)^\perp\text{.}\)</p></li>
<li id="li-304"><p id="p-1300">\(\NS(A^T)=\left(\CS(A)\right)^\perp\text{,}\) and thus \(\CS(A)=\left(\NS(A^T)\right)^\perp\text{.}\)</p></li>
</ol></article><article class="hiddenproof" id="proof-59"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-59"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-59"><article class="hiddenproof"><p id="p-1301">(i) Using the dot product method of matrix multiplication, we see that a vector \(\boldv\in\NS(A)\) if and only if \(\boldv\cdot\boldr_i=0\) for each row \(\boldr_i\) of \(A\text{.}\) Since the \(\boldr_i\) span \(\RS(A)\text{,}\) the linear properties of the dot product imply that \(\boldv\cdot\boldr_i=0\) for each row \(\boldr_i\) of \(A\) if and only if \(\boldv\cdot\boldw=0\) for <em class="emphasis">all</em> \(\boldw\in\RS(A)\) if and only if \(\boldv\in \RS(A)^\perp\text{.}\)</p>
<p id="p-1302">(ii) This follows from (i) and the fact that \(\CS(A)=\RS(A^T)\text{.}\)</p></article></div></article><article class="paragraphs" id="paragraphs-279"><h5 class="heading"><span class="title">Visualizing the rank-nullity theorem.</span></h5>
<div class="displaymath">
\begin{equation*}

\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-280"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1303">Understanding the orthogonal relationship between \(\NS(A)\) and \(\RS(A)\) allows us in many cases to quickly determine/visualize the one from the other. Consider the example \(A=\begin{bmatrix}1\amp -1\amp 1\\ 1\amp -1\amp -1 \end{bmatrix}\text{.}\)</p>
<p id="p-1304">Looking at the columns, we see easily that \(\rank(A)=2\text{,}\) which implies that \(\nullity(A)=3-2=1\text{.}\) Since \((1,-1,0)\) is an element of \(\NS(A)\) and \(\dim(\NS(A))=1\text{,}\) we must have \(\NS(A)=\Span\{(1,-1,0)\}\text{,}\) a line.</p>
<p id="p-1305">By orthogonality, we conclude that</p>
<div class="displaymath">
\begin{equation*}
\RS(A)=\NS(A)^\perp=\text{ (plane perpendicular to \((1,-1,0)\)) }\text{.}
\end{equation*}
</div>
<div class="displaymath">
\begin{equation*}

\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-281"><h5 class="heading"><span class="title">Orthogonal Projection.</span></h5>
<article class="theorem theorem-like" id="theorem-60"><h6 class="heading">
<span class="type">Theorem</span><span class="space"> </span><span class="codenumber">5.2.10</span><span class="period">.</span><span class="space"> </span><span class="title">Orthogonal projection theorem.</span>
</h6>
<p id="p-1306">Let \((V,\langle \ , \rangle)\) be an inner product space, and let \(W\subseteq V\) be a <em class="emphasis">finite-dimensional</em> subspace.</p>
<ol class="decimal">
<li id="li-305"><p id="p-1307">(Orthogonal decomposition). For all \(\boldv\in V\) there is a unique choice of vectors \(\boldw\in W\) and \(\boldw^\perp\in W^\perp\) such that \(\boldv=\boldw+\boldw^\perp\text{.}\) We call this vector expression an <dfn class="terminology">orthogonal decomposition</dfn> of \(\boldv\text{,}\) and denote \(\boldw=\proj{\boldv}{W}\) and \(\boldw^\perp=\proj{\boldv}{W^\perp}\text{,}\) the <dfn class="terminology">orthogonal projections</dfn> of \(\boldv\) onto \(W\) and \(W^\perp\text{,}\) respectively.</p></li>
<li id="li-306"><p id="p-1308">The orthogonal projection \(\boldw=\proj{\boldv}{W}\) is the element of \(W\) \alert{closest} to \(\boldv\text{.}\) This means \(\norm{\boldv-\proj{\boldv}{W}}\leq\norm{\boldv-\boldw'}\) for any other \(\boldw'\in W\text{.}\) We thus define the distance from \(\boldv\) to \(W\) as \(d(\boldv, W)=\norm{\boldv-\proj{\boldv}{W}}=\norm{\boldw^\perp}\text{.}\)</p></li>
<li id="li-307"><p id="p-1309">(Orthogonal projection formula). Pick an <em class="emphasis">orthogonal</em> basis \(B=\{\boldv_1,\boldv_2,\dots, \boldv_r\}\) of \(W\text{.}\) Then \(\ds \proj{\boldv}{W}=\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i\text{.}\)</p></li>
</ol></article></article><article class="paragraphs" id="paragraphs-282"><h5 class="heading"><span class="title">Proof of orthogonal projection theorem.</span></h5>
<p id="p-1310">Pick an <em class="emphasis">orthogonal</em> basis \(B=\{\boldv_1,\boldv_2,\dots, \boldv_r\}\) of \(W\) and set \(\boldw=\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i\text{.}\) This is clearly an element of \(W\text{.}\) Next we set \(\boldw^\perp=\boldv-\boldw=\boldv-\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i\text{.}\)</p>
<p id="p-1311">To complete the proof, we must show the following: (A) \(\boldw^\perp\in W^\perp\text{,}\) (B) this choice of \(\boldw\) and \(\boldw^\perp\) is unique, and (C) \(\boldw\) is the closest element of \(W\) to \(\boldv\text{.}\)</p>
<article class="paragraphs" id="paragraphs-283"><h5 class="heading"><span class="title">(A).</span></h5>
<p id="p-1312">For all \(i\) we have</p></article><div class="displaymath">
\begin{align*}
\langle\boldw^\perp,\boldv_i\rangle\amp =\amp \langle \boldv-\sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i, \boldv_i\rangle\\
\amp =\amp \langle \boldv, \boldv_i\rangle-\langle \sum_{i=1}^r\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i, \boldv_i}}\boldv_i ,\boldv_i\rangle \hspace{9pt} \text{ (distr.) }\\
\amp =\amp \langle \boldv, \boldv_i\rangle-\frac{\angvec{\boldv,\boldv_i}}{\angvec{\boldv_i,\boldv_i}}\langle\boldv_i,\boldv_i\rangle \hspace{9pt} \text{ (by orthogonality) }\\
\amp =\amp 0
\end{align*}
</div>
<article class="paragraphs" id="paragraphs-284"><h5 class="heading"><span class="title">(B)+(C).</span></h5>
<p id="p-1313">Recall: \(\boldw\) satisfies \(\boldv=\boldw+\boldw^\perp\text{,}\) where \(\boldw^\perp\in W^\perp\text{.}\) Now take any other \(\boldw'\in W\text{.}\) Then</p></article><div class="displaymath">
\begin{align*}
\norm{\boldv-\boldw'}^2\amp =\amp \norm{\boldw^\perp+(\boldw-\boldw')}^2
=\norm{\boldw^\perp}^2+\norm{\boldw-\boldw'}^2 \hspace{9pt} \text{ (Pythag. theorem) }\\
\amp \geq\amp  \norm{\boldw^\perp}^2=\norm{\boldv-\boldw}^2\text{.}
\end{align*}
</div>
<p id="p-1314">Taking square-roots now proves the desired inequality. Furthermore, we have <em class="emphasis">equality</em> iff the last inequality is an equality iff \(\norm{\boldw''}=\norm{\boldw-\boldw'}=0\) iff \(\boldw=\boldw'\text{.}\) This proves our choice of \(\boldw\) is the <em class="emphasis">unique</em> element of \(W\) minimizing the distance to \(\boldv\text{!}\)</p></article><article class="corollary theorem-like" id="corollary-11"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">5.2.11</span><span class="period">.</span>
</h6>
<p id="p-1315">Let \((V,\angvec{\ , \ })\) be an inner product space, and let \(W\subseteq V\) be a finite-dimensional subspace. Then \((W^\perp)^\perp=W\text{.}\)</p></article><article class="hiddenproof" id="proof-60"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-60"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-60"><article class="hiddenproof"><p id="p-1316">Clearly \(W\subseteq (W^\perp)^\perp\text{.}\) For the other direction, take \(\boldv\in (W^\perp)^\perp\text{.}\) Using the <em class="emphasis">orthogonal projection theorem</em>, we can write \(\boldv=\boldw+\boldw^\perp\) with \(\boldw\in W\) and \(\boldw^\perp\in W^\perp\text{.}\) We will show \(\boldw^\perp=\boldzero\text{.}\)</p>
<p id="p-1317">Since \(\boldv\in (W^\perp)^\perp\) we have \(\angvec{\boldv,\boldw^\perp}=0\text{.}\) Then we have</p>
<div class="displaymath">
\begin{align*}
0\amp =\angvec{\boldv,\boldw^\perp}\\
\amp =\angvec{\boldw+\boldw^\perp,\boldw^\perp}\\
\amp =\angvec{\boldw,\boldw^\perp}+\angvec{\boldw^\perp,\boldw^\perp} \amp \text{ (since \(W\perp W^\perp\)) }\\
\amp =0+\angvec{\boldw^\perp,\boldw^\perp}
\end{align*}
</div>
<p id="p-1318">Thus \(\angvec{\boldw^\perp,\boldw^\perp}=0\text{.}\) It follows that \(\boldw^\perp=\boldzero\text{,}\) and hence \(\boldv=\boldw+\boldzero=\boldw\in W\text{.}\)</p></article></div>
<article class="corollary theorem-like" id="corollary-12"><h6 class="heading">
<span class="type">Corollary</span><span class="space"> </span><span class="codenumber">5.2.12</span><span class="period">.</span>
</h6>
<p id="p-1319">Let \((V,\angvec{\ , \ })\) be an inner product space, and let \(W\subseteq V\) be a finite-dimensional subspace.</p>
<p id="p-1320">Define \(T\colon V\rightarrow V\) as \(T(\boldv)=\proj{\boldv}{W}\text{.}\) Then \(T\) is a linear transformation.</p>
<p id="p-1321">In other words, orthogonal projection onto \(W\) defines a linear transformation of \(V\text{.}\)</p></article><article class="hiddenproof" id="proof-61"><a data-knowl="" class="id-ref proof-knowl original" data-refid="hk-proof-61"><h6 class="heading"><span class="type">Proof<span class="period">.</span></span></h6></a></article><div class="hidden-content tex2jax_ignore" id="hk-proof-61"><article class="hiddenproof"><p id="p-1322">We must show that \(T(c\boldv_1+d\boldv_2)=cT(\boldv_1)+dT(\boldv_2)\) for all \(c,d\in\R\) and \(\boldv_1,\boldv_2\in V\text{.}\) This is easily shown by picking an orthonormal basis \(B=\{\boldv_1,\boldv_2, \dots, \boldv_r\}\) of \(W\) and using the formula from the orthogonal projection theorem.</p></article></div>
<article class="paragraphs" id="paragraphs-285"><h5 class="heading"><span class="title">Projection onto lines and planes in $\R^3$.</span></h5>
<p id="p-1323">Let's revisit orthogonal projection onto lines and planes in \(\R^3\) passing through the origin. Here the relevant inner product is dot product.</p></article><article class="paragraphs" id="paragraphs-286"><h5 class="heading"><span class="title">Projection onto a line $\ell$.</span></h5>
<p id="p-1324">Any line in \(\R^3\) passing through the origin can be described as \(\ell=\Span\{\boldv_0\}\text{,}\) for some \(\boldv_0=(a,b,c)\ne 0\text{.}\) Since this is an orthogonal basis of \(\ell\text{,}\) by the orthogonal projection theorem we have, for any \(\boldv=(x,y,z)\)</p>
<div class="displaymath">
\begin{equation*}
\proj{\boldv}{\ell}=\frac{\boldv\cdot \boldv_0}{\boldv_0\cdot\boldv_0}\boldv_0=\frac{ax+by+cz}{a^2+b^2+c^2}(a,b,c)=\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix} \begin{bmatrix}x\\ y\\ z \end{bmatrix}\text{.}
\end{equation*}
</div>
<p id="p-1325">We have re-derived the matrix formula for orthogonal projection onto \(\ell\text{.}\)</p></article><article class="paragraphs" id="paragraphs-287"><h5 class="heading"><span class="title">Projection onto lines and planes in $\R^3$.</span></h5>
<p id="p-1326">Let's revisit orthogonal projection onto lines and planes in \(\R^3\) passing through the origin. Here the relevant inner product is dot product.</p></article><article class="paragraphs" id="paragraphs-288"><h5 class="heading"><span class="title">Projection onto a plane.</span></h5>
<p id="p-1327">Any plane in \(\R^3\) passing through the origin can be described with the equation \(\mathcal{P}\colon ax+by+cz=0\) for some \(\boldn=(a,b,c)\ne 0\text{.}\) This says precisely that \(\mathcal{P}\) is the orthogonal complement of the line \(\ell=\Span\{(a,b,c)\}\text{:}\) i.e., \(\mathcal{P}=\ell^\perp\text{.}\)</p>
<p id="p-1328">From the orthogonal projection theorem, we know that</p>
<div class="displaymath">
\begin{equation*}
\boldv=\proj{\boldv}{\ell}+\proj{\boldv}{\ell^\perp}=\proj{\boldv}{\ell}+\proj{\boldv}{\mathcal{P}}\text{.}
\end{equation*}
</div>
<p id="p-1329">But then</p>
<div class="displaymath">
\begin{equation*}
\proj{\boldv}{\mathcal{P}}=\boldv-\proj{\boldv}{\ell}=I \ \boldv-\proj{\boldv}{\ell}=(I-A)\boldv\text{,}
\end{equation*}
</div>
<p class="continuation">where \(A\) is the matrix formula for \(\proj{\boldv}{\ell}\) from the previous example. We conclude that the matrix defining \(\proj{\boldv}{\mathcal{P}}\) is</p>
<div class="displaymath">
\begin{equation*}
I-\frac{1}{a^2+b^2+c^2}\begin{bmatrix}a^2\amp ab\amp ac\\ ab\amp b^2\amp bc\\ ac\amp bc\amp c^2 \end{bmatrix} = \frac{1}{a^2+b^2+c^2}\begin{bmatrix}b^2+c^2\amp -ab\amp -ac\\ -ab\amp a^2+c^2\amp -bc\\ -ac\amp -bc\amp a^2+b^2 \end{bmatrix}
\end{equation*}
</div></article><p id="p-1330">We can express this in terms of matrix multiplication as</p>
<p id="p-1331">\item Translate the whole picture by \(-Q=(-q_1,-q_2, -q_3)\text{,}\) which means we replace \(P=(x,y,z)\) with \(P-Q=(x-q_1,y-q_2,z-q_3)\text{.}\) \item Apply our formulas from before, replacing \((x,y,z)\) with \((x-q_1,y-q_2,z-q_3)\) \item Translate back by adding \(Q\) to your answer.</p>
<article class="paragraphs" id="paragraphs-289"><h5 class="heading"><span class="title">Example: sine/cosine series.</span></h5>
<p id="p-1332">Let \(V=C[0,2\pi]\) with inner product \(\langle f, g\rangle=\int_0^{2\pi}f(x)g(x) \, dx\text{.}\)</p>
<p id="p-1333">We have seen that the set</p>
<div class="displaymath">
\begin{equation*}
B=\{1, \cos(x),\sin(x),\cos(2x),\sin(2x), \dots , \cos(nx),\sin(nx)\}
\end{equation*}
</div>
<p class="continuation">is orthogonal. Thus \(B\) is an orthogonal basis of \(W=\Span(B)\text{,}\) which we might describe as the space of <dfn class="terminology">trigonometric polynomials of degree at most \(n\)</dfn>.</p>
<p id="p-1334">Given an arbitrary function \(f(x)\in C[0,2\pi]\text{,}\) its orthogonal projection onto \(W\) is the function</p>
<div class="displaymath">
\begin{equation*}
\hat{f}(x)=a_0+a_1\cos(x)+b_1\sin(x)+a_2\cos(2x)+b_2\sin(2x)+\cdots +a_n\cos(nx)+b_n\sin(nx)\text{,}
\end{equation*}
</div>
<p class="continuation">where</p>
<div class="displaymath">
\begin{equation*}
a_0=\frac{1}{2\pi}\int_0^{2\pi} f(x) \ dx, \ a_j=\frac{1}{\pi}\int_0^{2\pi}f(x)\cos(jx)\, dx, \ b_k=\frac{1}{\pi}\int_0^{2\pi}f(x)\sin(kx)\, dx\text{.}
\end{equation*}
</div>
<p id="p-1335">The projection theorem tells us that \(\hat{f}\) is the “best” trigonometric polynomial approximation of \(f(x)\) (of degree at most \(n\)), in the sense that for any other sinusoidal \(g\in W\text{,}\) \(\left\vert\left\vert f-\hat{f}\right\vert\right\vert\leq \norm{f-g}\text{.}\)</p>
<p id="p-1336">This means in turn</p>
<div class="displaymath">
\begin{equation*}
\int_0^{2\pi} (f-\hat{f})^2\, dx\leq \int_0^{2\pi} (f-g)^2 \, dx\text{.}
\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-290"><h5 class="heading"><span class="title">Example: least-squares solution to $A\boldx=\boldy$.</span></h5>
<p id="p-1337">Often in applications we have an \(m\times n\) matrix \(A\) and vector \(\boldy\in\R^m\) for which the matrix equation</p>
<div class="displaymath">
\begin{equation*}
A\boldx=\boldy
\end{equation*}
</div>
<p class="continuation">has no solution. In terms of fundamental spaces, this means simply that \(\boldy\notin \CS(A)\text{.}\) Set \(W=\CS(A)\text{.}\)</p>
<p id="p-1338">In such situations we speak of a <em class="emphasis">least-squares</em> solution to the matrix equation. This is a vector \(\hat{\boldx}\) such that \(A\hat{\boldx}=\hat{\boldy}\text{,}\) where \(\hat{\boldy}=\proj{\boldy}{W}\text{.}\) Here the inner product is taken to be the dot product.</p>
<p id="p-1339">Note: the equation \(A\hat{\boldx}=\hat{\boldy}\) is guaranteed to have a solution since \(\hat{\boldy}=\proj{\boldy}{W}\) lies in \(\CS(A)\text{.}\)</p>
<p id="p-1340">The vector \(\hat{\boldx}\) is called a least-square solutions because its image \(\hat{\boldy}\) is the element of \(\CS(A)\) that is “closest” to \(\boldy\) in terms of the dot product. Writing \(\boldy=(y_1,y_2,\dots,y_n)\) and \(\hat{\boldy}=(y_1',y_2',\dots,
y_n')\text{,}\) this means that \(\hat{\boldy}\) minimizes the distance</p>
<div class="displaymath">
\begin{equation*}
\norm{\boldy-\hat{\boldy}}=\sqrt{(y_1-y_1')^2+(y_2-y_2')^2+\cdots +(y_n-y_n')^2}\text{.}
\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-291"><h5 class="heading"><span class="title">Least-squares example (curve fitting).</span></h5>
<p id="p-1341">Suppose we wish to find an equation of a line \(y=mx+b\) that best fits (in the least-square's sense) the following \((x,y)\) data points: \(P_1=(-3,1), P_2=(1,2), P_3=(2,3)\text{.}\)</p>
<p id="p-1342">Then we seek \(m\) and \(b\) such that</p>
<div class="displaymath">
\begin{align*}
1\amp =m(-3)+b\\
2\amp =m(1)+b\\
3\amp =m(2)+b\text{,}
\end{align*}
</div>
<p class="continuation">or equivalently, we wish to solve \(\begin{bmatrix}-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{bmatrix} \begin{bmatrix}m \\ b \end{bmatrix} =\begin{bmatrix}1\\ 2\\ 3 \end{bmatrix}\text{.}\)</p>
<p id="p-1343">This equation has no solution as \(\boldy=(1,2,3)\) does no lie in \(W=\CS(A)=\Span(\{(-3,1,2),(1,1,1)\}\text{.}\) So instead we compute \(\hat{\boldy}=\proj{\boldy}{W}=(13/14,33/14,38/14)\text{.}\) (This was not hard to compute as conveniently the given basis of \(W\) was already orthogonal!)</p>
<p id="p-1344">Finally we solve \(A\begin{bmatrix}m\\ b \end{bmatrix} =\hat{\boldy}\text{,}\) getting \(m=5/14\text{,}\) \(b=28/14=2\text{.}\) Thus \(y=\frac{5}{14}x+2\) is the line best fitting the data in the least-squares sense.</p></article><article class="paragraphs" id="paragraphs-292"><h5 class="heading"><span class="title">Least-squares example contd..</span></h5>
<p id="p-1345">In what sense does \(y=\frac{5}{14}x+2\) “best” fit the data?</p>
<div class="displaymath">
\begin{equation*}

\end{equation*}
</div>
<p id="p-1346">Let \(\boldy=(1,2,3)=(y_1,y_2,y_3)\) be the given \(y\)-values of the points, and \(\hat{\boldy}=(y_1',y_2',y_3')\) be the projection we computed before. In the graph the values \(\epsilon_i\) denote the vertical difference \(\epsilon_i=y_i-y_i'\) between the data points, and our fitting line.</p>
<p id="p-1347">The projection \(\hat{\boldy}\) makes the error \(\norm{\boldy-\hat{\boldy}}=\sqrt{ \epsilon_1^2+\epsilon_2^2+\epsilon_3^2}\) as small as possible.</p>
<p id="p-1348">This means if I draw <em class="emphasis">any other line</em> and compute the corresponding differences \(\epsilon_i'\) at the \(x\)-values -3, 1 and 2, then we have</p>
<div class="displaymath">
\begin{equation*}
\epsilon_1^2+\epsilon_2^2+\epsilon_3^2\leq (\epsilon_1')^2+(\epsilon_2')^2+(\epsilon_3')^2
\end{equation*}
</div></article><article class="paragraphs" id="paragraphs-293"><h5 class="heading"><span class="title">Finding least squares solutions.</span></h5>
<p id="p-1349">As the last example illustrated, one method of finding a least-squares solution \(\boldx\) to \(A\boldx=\boldy\) is to first produce an orthogonal basis for \(\CS(A)\text{,}\) then compute \(\hat{\boldy}=\proj{\boldy}{\CS(A)}\text{,}\) and then use GE to solve \(A\boldx=\hat{\boldy}\text{.}\)</p>
<p id="p-1350">Alternatively, it turns out (through a little trickery) that \(\hat{\boldy}=A\boldx\text{,}\) where \(\boldx\) is a solution to the equation</p>
<div class="displaymath">
\begin{equation*}
A^TA\boldx=A^T\boldy\text{.}
\end{equation*}
</div>
<p id="p-1351">This solves us the hassle of computing an orthogonal basis for \(\CS(A)\text{;}\) to find a least-squares solution \(\boldx\) for \(A\boldx=\boldy\text{,}\) we simply use GE to solve the boxed equation. (Some more trickery shows a solution is guaranteed to exist!)</p>
<article class="paragraphs" id="paragraphs-294"><h5 class="heading"><span class="title">Example.</span></h5>
<p id="p-1352">In the previous example we were seeking a least-squares solution \(\boldx=\colvec{m\\ b}\) to \(A\boldx=\boldy\text{,}\) where \(A=\begin{bmatrix}-3\amp 1\\ 1\amp 1\\ 2\amp 1 \end{bmatrix} , \boldy=\colvec{1\\2\\3}\text{.}\)</p></article><p id="p-1353">The equation \(A^TA\boldx=A^T\boldy\) is thus</p>
<div class="displaymath">
\begin{equation*}
\begin{bmatrix}14\amp 0\\ 0\amp 3 \end{bmatrix} \boldx= \colvec{5\\ 6}
\end{equation*}
</div>
<p id="p-1354">As you can see, \(\boldx=\colvec{m\\ b}=\colvec{5/14\\ 2}\) is a least-squares solution, just as before</p></article></section></div></main>
</div>
</body>
</html>
